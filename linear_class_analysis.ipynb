{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e160ba7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62dba84",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 300\n",
    "\n",
    "data_name = \"triplet_digits_2\"\n",
    "embedding_dims = [32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]\n",
    "margins = [3., 4., 5.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c6d506",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = []\n",
    "\n",
    "for ed, margin in itertools.product(embedding_dims, margins):\n",
    "    margin_str = f\"{margin:.1f}\".replace('.', 'p')\n",
    "    hyper_path = f\"models/class_{data_name}_hyper_M{margin_str}_ED{ed}_EP{epochs}.pkl\"\n",
    "\n",
    "    if not os.path.exists(hyper_path):\n",
    "        print(f\" File not found: {hyper_path}\")\n",
    "        continue\n",
    "\n",
    "    with open(hyper_path, 'rb') as f:\n",
    "        saved = pickle.load(f)\n",
    "    \n",
    "    hyper_params.append({\n",
    "        \"model id\": saved[\"model id\"],\n",
    "        \"learning rate\": saved[\"learning rate\"],\n",
    "        \"weight decay\": saved[\"weight decay\"],\n",
    "        \"batch size\": saved[\"batch size\"],\n",
    "        \"embedding dim\": saved[\"embedding dim\"],  \n",
    "        \"l1 lambda\": saved[\"l1 lambda\"],\n",
    "        \"epochs\": saved[\"epochs\"],  \n",
    "        \"margin\": saved[\"margin\"],  \n",
    "        \"patience\": saved[\"patience\"],\n",
    "        \"min delta\": saved[\"min delta\"],\n",
    "        })\n",
    "n_models = len(hyper_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafdde37",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for hp in hyper_params:  # assuming hyper_params is a list of dicts with 'hidden dim' and 'embedding dim'\n",
    "    margin = hp['margin']\n",
    "    embedding_dim = hp['embedding dim']\n",
    "    margin_str = f\"{margin:.1f}\".replace('.', 'p')\n",
    "    filename = f\"models/class_{data_name}_M{margin_str}_ED{embedding_dim}_EP{epochs}.pkl\"\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\" File not found: {filename}\")\n",
    "        continue\n",
    "    with open(filename, 'rb') as f:\n",
    "        saved_train = pickle.load(f)\n",
    "        \n",
    "    filename = f\"models/class_test_{data_name}_M{margin_str}_ED{embedding_dim}_EP{epochs}.pkl\"\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\" File not found: {filename}\")\n",
    "        continue\n",
    "    with open(filename, 'rb') as f:\n",
    "        saved_test = pickle.load(f)\n",
    "\n",
    "    results.append({\n",
    "        \"train_losses\": saved_train[\"train_losses\"],\n",
    "        \"val_losses\": saved_train[\"val_losses\"],\n",
    "        \"train_viol\": saved_train[\"train_viol\"],\n",
    "        \"val_viol\": saved_train[\"val_viol\"],\n",
    "        \"train_time\": saved_train[\"train_time\"],\n",
    "        \"best_epoch\": saved_train[\"best_epoch\"],\n",
    "        \"test_losses\": saved_test[\"test_loss\"],\n",
    "        \"test_viol\": saved_test[\"test_viol\"],\n",
    "        \"test_gini\": saved_test[\"test_gini\"],\n",
    "    })\n",
    "\n",
    "print(f\"âœ… Loaded {len(results)} models from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429c6ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Get all unique margins ---\n",
    "margins = sorted(set(hp[\"margin\"] for hp in hyper_params))\n",
    "\n",
    "best_m_id = []\n",
    "\n",
    "print(\"\\n Best model for each margin:\")\n",
    "for m in margins:\n",
    "    # All models with this margin\n",
    "    margin_ids = [i for i, hp in enumerate(hyper_params) if hp[\"margin\"] == m]\n",
    "    \n",
    "    # Pick the one with minimum validation loss\n",
    "    best_id = min(margin_ids, key=lambda i: min(results[i][\"val_losses\"]))\n",
    "    best_m_id.append(best_id)\n",
    "    \n",
    "    # Print summary for this margin\n",
    "    best_loss = min(results[best_id][\"val_losses\"])\n",
    "    ed = hyper_params[best_id]['embedding dim']\n",
    "    print(f\"Margin={m}: Model ID={best_id}, ED={ed}, Best Validation Loss={best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e646d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sort and pick best based on early stopping ---\n",
    "best_id = min(range(len(results)), key=lambda i: min(results[i][\"val_losses\"]))\n",
    "best_result = results[best_id]\n",
    "best_hyperparams = hyper_params[best_id]\n",
    "best_val_loss = min(best_result[\"val_losses\"])\n",
    "best_val_viol = min(best_result[\"val_viol\"])\n",
    "\n",
    "print(\"\\n Best config:\")\n",
    "print(f\"   ED={best_hyperparams['embedding dim']}, \"\n",
    "    f\"M={best_hyperparams['margin']}\")\n",
    "print(f\"   Best Validation Loss={best_val_loss:.4f}\")\n",
    "print(f\"   Training Time: {best_result['train_time']:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefd50a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_i = [\"viol\",\"losses\",\"gini\"]\n",
    "embedding_exp = ['1/8', '1/4', '1/2', '1', '2', '4']\n",
    "hidden_exp = ['1/8', '1/4', '1/2', '1', '2', '4', '8', '16', '32']\n",
    "#metric = \"viol\"\n",
    "#metric = \"losses\"\n",
    "#metric = \"gini\"\n",
    "mouse_exp = 20*16*16\n",
    "human_exp = 300*16*16\n",
    "\n",
    "xvar = 'embedding dim'\n",
    "yvar = 'margin'\n",
    "xvals = sorted(set(r[xvar] for r in hyper_params))\n",
    "yvals = sorted(set(r[yvar] for r in hyper_params))\n",
    "\n",
    "# Define styles\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple']\n",
    "yvals_to_color = {ed: colors[i % len(colors)] for i, ed in enumerate(yvals)}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(9, 5))\n",
    "ip = 0\n",
    "for metric in metric_i:\n",
    "    \n",
    "    \n",
    "    xval_array = np.array(xvals)\n",
    "\n",
    "    for yval in yvals:\n",
    "        ip += 1\n",
    "        plt.subplot(3,5,ip)\n",
    "\n",
    "        test_losses = []\n",
    "        for xval in xvals:\n",
    "            model_id = next(i for i, r in enumerate(hyper_params)\n",
    "                            if r[xvar] == xval and r[yvar] == yval)\n",
    "\n",
    "            test_losses.append(results[model_id]['test_'+metric])\n",
    "\n",
    "        color = yvals_to_color[yval]\n",
    "        label = f\"{yvar}={yval}\"\n",
    "        plt.plot(xval_array, test_losses, 'o-',label=label,alpha=0.5)\n",
    "        yl = plt.ylim()  # get current y-axis limits\n",
    "        plt.plot(mouse_exp*np.ones(2), yl, '--r', linewidth=2)  # vertical line at x=3\n",
    "        #plt.plot(human_exp*np.ones(2), yl, '-.g', linewidth=2)  # vertical line at x=7\n",
    "        # Formatting\n",
    "        plt.xscale(\"log\")\n",
    "        plt.xlabel(xvar)\n",
    "        plt.ylabel(metric)\n",
    "        plt.title(label)\n",
    "        #plt.legend()\n",
    "        plt.tight_layout()\n",
    "#plt.savefig('figures/test_results.png')\n",
    "xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9df3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_i = [\"viol\",\"losses\",\"gini\"]\n",
    "#metric = \"viol\"\n",
    "#metric = \"losses\"\n",
    "#metric = \"gini\"\n",
    "mouse_exp = 20*16*16\n",
    "human_exp = 300*16*16\n",
    "\n",
    "xvar = 'embedding dim'\n",
    "yvar = 'margin'\n",
    "xvals = sorted(set(r[xvar] for r in hyper_params))\n",
    "yvals = sorted(set(r[yvar] for r in hyper_params))\n",
    "nx = len(xvals)\n",
    "ny = len(yvals)\n",
    "nm = len(metric_i)\n",
    "\n",
    "# Define styles\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple']\n",
    "yvals_to_color = {ed: colors[i % len(colors)] for i, ed in enumerate(yvals)}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(ny*2, nm*2))\n",
    "ip = 0\n",
    "for metric in metric_i:\n",
    "    \n",
    "    \n",
    "    xval_array = np.array(xvals)\n",
    "\n",
    "    for yval in yvals:\n",
    "        ip += 1\n",
    "        plt.subplot(nm,ny,ip)\n",
    "\n",
    "        test_losses = []\n",
    "        for xval in xvals:\n",
    "            model_id = next(i for i, r in enumerate(hyper_params)\n",
    "                            if r[xvar] == xval and r[yvar] == yval)\n",
    "\n",
    "            test_losses.append(results[model_id]['test_'+metric])\n",
    "\n",
    "        color = yvals_to_color[yval]\n",
    "        label = f\"{yvar}={yval}\"\n",
    "        plt.plot(xval_array, test_losses, 'o-',label=label,alpha=0.5)\n",
    "        yl = plt.ylim()  # get current y-axis limits\n",
    "        #plt.plot(mouse_exp*np.ones(2), yl, '--r', linewidth=2)  # vertical line at x=3\n",
    "        \n",
    "        # Formatting\n",
    "        plt.xscale(\"log\")\n",
    "        plt.xlabel(xvar)\n",
    "        plt.ylabel(metric)\n",
    "        plt.title(label)\n",
    "        #plt.legend()\n",
    "        plt.tight_layout()\n",
    "plt.savefig(f'figures/test_results_{data_name}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c040a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx\n",
    "# Extract unique values\n",
    "hds = sorted(set(r['margin'] for r in hyper_params))\n",
    "eds = sorted(set(r['embedding dim'] for r in hyper_params))\n",
    "\n",
    "n_hds = len(hds)\n",
    "n_eds = len(eds)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "hd_array = np.array(hds)\n",
    "ed_array = np.array(eds)\n",
    "\n",
    "test_loss = np.zeros((n_eds,n_hds))\n",
    "test_viol = np.zeros((n_eds,n_hds))\n",
    "test_gini = np.zeros((n_eds,n_hds))\n",
    "best_epoch = np.zeros((n_eds,n_hds),dtype=int)\n",
    "train_time = np.zeros((n_eds,n_hds))\n",
    "for ied,ed in enumerate(eds):\n",
    "    \n",
    "    val_losses = []\n",
    "    for ihd,hd in enumerate(hds):\n",
    "        model_id = next(i for i, r in enumerate(hyper_params)\n",
    "                        if r['embedding dim'] == ed and \n",
    "                        r['margin'] == hd)\n",
    "\n",
    "        test_loss[ied,ihd] = results[model_id]['test_losses']\n",
    "        test_viol[ied,ihd] = results[model_id]['test_viol']\n",
    "        test_gini[ied,ihd] = results[model_id]['test_gini']\n",
    "        best_epoch[ied,ihd] = results[model_id]['best_epoch']\n",
    "        train_time[ied,ihd] = results[model_id]['train_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce901b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange_blocks_fixed(blocks, p, q):\n",
    "    m, n1, n2 = blocks.shape\n",
    "    assert n1 == n2, \"Blocks must be square\"\n",
    "    assert m == p * q, \"m must equal p * q\"\n",
    "    n = n1\n",
    "    reshaped = blocks.reshape(p, q, n, n)\n",
    "    merged = reshaped.swapaxes(1, 2).reshape(p * n, q * n)\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0891b24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resmooth_weights(model, sigma=0.25):\n",
    "    weight = model.fc.weight.data\n",
    "    emb_dim = weight.shape[0]\n",
    "    weight_reshaped = weight.view(emb_dim, 1, 16, 16)\n",
    "\n",
    "    kernel_size = int(2 * round(3 * sigma) + 1)\n",
    "    kernel = gaussian_kernel2d(kernel_size, sigma, device=weight.device)\n",
    "    kernel = kernel.view(1, 1, kernel_size, kernel_size)\n",
    "\n",
    "    smoothed = F.conv2d(weight_reshaped, kernel, padding=kernel_size // 2)\n",
    "    model.fc.weight.data.copy_(smoothed.view(emb_dim, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d689d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self, embedding_dim=32):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(16*16, embedding_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        out = self.relu(self.fc(x))  # Linear followed by ReLU\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50d5836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel2d(kernel_size=5, sigma=0.25, device='cpu'):\n",
    "    ax = torch.arange(kernel_size, device=device) - kernel_size // 2\n",
    "    xx, yy = torch.meshgrid(ax, ax, indexing=\"xy\")\n",
    "    kernel = torch.exp(-(xx**2 + yy**2) / (2 * sigma**2))\n",
    "    kernel /= kernel.sum()\n",
    "    return kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797ab847",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Load best model\n",
    "weights_reshaped = []\n",
    "for model_id in range(n_models):\n",
    "\n",
    "    ed = hyper_params[model_id][\"embedding dim\"]\n",
    "\n",
    "    margin = hyper_params[model_id][\"margin\"]\n",
    "    margin_str = f\"{margin:.1f}\".replace('.', 'p')\n",
    "    model_file = f\"models/class_{data_name}_M{margin_str}_ED{ed}_EP{epochs}.pt\"\n",
    "    model = EmbeddingNet(embedding_dim=ed)\n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # ---- Smooth the weights again ----\n",
    "    resmooth_weights(model, sigma=.5)\n",
    "\n",
    "    # Extract weights from first layer (input to hidden): shape (n_hidden_units, 256)\n",
    "    first_layer = model.fc  # Replace if you used a different name\n",
    "    weights = first_layer.weight.detach().cpu().numpy()  # shape: (n_hidden_units, 256)\n",
    "\n",
    "    # Reshape to (n_hidden_units, 16, 16)\n",
    "    weights_reshaped.append(weights.reshape(weights.shape[0], 16, 16))\n",
    "\n",
    "    #print(\"Weight shape:\", weights_reshaped[model_id].shape)  # (n_hidden_units, 16, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d14a416",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 16\n",
    "p = 4 # rows \n",
    "q = 8 # columns\n",
    "size_factor = 0.5\n",
    "for model_id in range(n_models):\n",
    "    \n",
    "    embedding_dim = hyper_params[model_id]['embedding dim']\n",
    "    margin = hyper_params[model_id]['margin']\n",
    "    # ---- Filter ----\n",
    "    if margin != 4.0 or embedding_dim not in [32, 512, 4096]:\n",
    "        continue  # skip models that don't match criteria\n",
    "    \n",
    "    merged_rf = rearrange_blocks_fixed(weights_reshaped[model_id][:32,:,:], p=p, q=q)\n",
    "    plt.figure(figsize=(q*size_factor,p*size_factor))\n",
    "    plt.imshow(merged_rf)\n",
    "    for i in range(q+1):\n",
    "        plt.plot(i*n*np.ones(2)-0.5,np.array([0,n*p])-0.5,'w')\n",
    "    for i in range(p+1):\n",
    "        plt.plot(np.array([0,n*q])-0.5,i*n*np.ones(2)-0.5,'w')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    embedding_dim = hyper_params[model_id]['embedding dim']\n",
    "    margin = hyper_params[model_id]['margin']\n",
    "    #lambda_str = \"0\" if l1_lambda == 0 else f\"{l1_lambda:.0e}\".replace('-', 'm')\n",
    "    plt.title(f\"M={margin}, ED={embedding_dim}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'figures/RF_ED{embedding_dim}.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dea061",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors_np = np.load(f'models/anchors_data_{data_name}.npy')\n",
    "print(\"Shape:\", anchors_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584de748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "data = np.load(f\"models/emb_act_data_{data_name}_EP{epochs}.npz\")\n",
    "emb_act = [data[f\"arr_{i}\"] for i in range(len(data.files))]\n",
    "print(emb_act[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6b5aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "margin = 3.\n",
    "ed = 1024\n",
    "\n",
    "model_id = next(i for i, r in enumerate(hyper_params)\n",
    "                if r[\"embedding dim\"] == ed and\n",
    "                r[\"margin\"] == margin)\n",
    "print(model_id)\n",
    "print(emb_act[model_id].shape)\n",
    "\n",
    "ed = 32\n",
    "model_id_32 = next(i for i, r in enumerate(hyper_params)\n",
    "                if r[\"embedding dim\"] == ed and\n",
    "                r[\"margin\"] == margin)\n",
    "print(model_id_32)\n",
    "print(emb_act[model_id_32].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7dc652",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# --- Fit PCA ---\n",
    "pca0 = PCA(n_components=2)  # 2D projection\n",
    "pca0.fit(anchors_np)  # shape: (20000, 2)\n",
    "anchors_pca = pca0.transform(anchors_np)  # shape: (20000, 2)\n",
    "\n",
    "# --- Explained variance ---\n",
    "print(\"Explained variance ratio:\", pca0.explained_variance_ratio_)\n",
    "print(\"Total variance explained:\", pca0.explained_variance_ratio_.sum())\n",
    "\n",
    "# --- Fit PCA ---\n",
    "pca32 = PCA(n_components=2)  # 2D projection\n",
    "pca32.fit(emb_act[model_id_32])  # shape: (20000, 2)\n",
    "emb_pca32 = pca32.transform(emb_act[model_id_32])  # shape: (20000, 2)\n",
    "\n",
    "# --- Explained variance ---\n",
    "print(\"Explained variance ratio:\", pca32.explained_variance_ratio_)\n",
    "print(\"Total variance explained:\", pca32.explained_variance_ratio_.sum())\n",
    "\n",
    "# --- Fit PCA ---\n",
    "pca = PCA(n_components=2)  # 2D projection\n",
    "pca.fit(emb_act[model_id])  # shape: (20000, 2)\n",
    "emb_pca = pca.transform(emb_act[model_id])  # shape: (20000, 2)\n",
    "\n",
    "# --- Explained variance ---\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
    "print(\"Total variance explained:\", pca.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3be343",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = np.load(f'models/test_labels_{data_name}.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229f6c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot ---\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1,3,1)\n",
    "scatter = plt.scatter(anchors_pca[:, 0], anchors_pca[:, 1], \n",
    "                      c=test_labels, cmap=\"tab10\", s=2)\n",
    "#plt.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "plt.title(\"PCA, 256 input dimensions\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "scatter = plt.scatter(emb_pca32[:, 0], emb_pca32[:, 1], \n",
    "                      c=test_labels, cmap=\"tab10\", s=2)\n",
    "#plt.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "plt.title(\"PCA, 32 embedding dimensions\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "scatter = plt.scatter(emb_pca[:, 0], emb_pca[:, 1], \n",
    "                      c=test_labels, cmap=\"tab10\", s=2)\n",
    "#plt.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "plt.title(\"PCA, 1024 embedding dimensions\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f'figures/PCA_{data_name}_M{margin_str}_ED{ed}.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043a796d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881c0fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_split_indices(n_samples, val_size=0.2, test_size=0.2, random_state=42):\n",
    "    # First split train+val and test\n",
    "    train_val_idx, test_idx = train_test_split(np.arange(n_samples),\n",
    "                                               test_size=test_size,\n",
    "                                               random_state=random_state,\n",
    "                                               shuffle=True)\n",
    "    # Split train and val\n",
    "    train_idx, val_idx = train_test_split(train_val_idx,\n",
    "                                          test_size=val_size/(1-test_size),\n",
    "                                          random_state=random_state,\n",
    "                                          shuffle=True)\n",
    "    return train_idx, val_idx, test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f465ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_softmax_classifier_pre_split(X_train, y_train, X_val, y_val,\n",
    "                                       lr=1e-3, weight_decay=1e-3,\n",
    "                                       epochs=2000, patience=20):\n",
    "    \"\"\"\n",
    "    Train a linear softmax classifier using pre-split data.\n",
    "    \"\"\"\n",
    "    # ---- Ensure tensors and correct dtypes ----\n",
    "    if not isinstance(X_train, torch.Tensor):\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    else:\n",
    "        X_train = X_train.float()\n",
    "\n",
    "    if not isinstance(X_val, torch.Tensor):\n",
    "        X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "    else:\n",
    "        X_val = X_val.float()\n",
    "\n",
    "    if not isinstance(y_train, torch.Tensor):\n",
    "        y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "    else:\n",
    "        y_train = y_train.long()\n",
    "\n",
    "    if not isinstance(y_val, torch.Tensor):\n",
    "        y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "    else:\n",
    "        y_val = y_val.long()\n",
    "\n",
    "    # ---- Model, Loss, Optimizer ----\n",
    "    model = LinearSoftmax(X_train.shape[1], len(torch.unique(y_train)))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # ---- Tracking ----\n",
    "    history = {\n",
    "        \"val_loss\": [], \"val_acc\": [],\n",
    "        \"train_loss\": [], \"train_acc\": []\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_state = None\n",
    "    wait = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # --- Train ---\n",
    "        model.train()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss = loss.item()\n",
    "        train_pred = outputs.argmax(dim=1)\n",
    "        train_acc = (train_pred == y_train).float().mean().item()\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs, y_val).item()\n",
    "            val_pred = val_outputs.argmax(dim=1)\n",
    "            val_acc = (val_pred == y_val).float().mean().item()\n",
    "\n",
    "        # ---- Store metrics ----\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        # --- Early stopping ---\n",
    "        if val_loss < best_val_loss - 1e-5:\n",
    "            best_val_loss = val_loss\n",
    "            best_state = model.state_dict()\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f51e47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSoftmax(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)  # softmax handled by loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80d4319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Compute test loss and accuracy for a trained model.\n",
    "    \"\"\"\n",
    "    # Ensure correct tensor types\n",
    "    if not isinstance(X_test, torch.Tensor):\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    else:\n",
    "        X_test = X_test.float()\n",
    "\n",
    "    if not isinstance(y_test, torch.Tensor):\n",
    "        y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "    else:\n",
    "        y_test = y_test.long()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test)\n",
    "        loss = criterion(outputs, y_test).item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        acc = (preds == y_test).float().mean().item()\n",
    "\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657aa5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
