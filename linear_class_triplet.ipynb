{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40855784-cf70-4126-b37c-cc86ad91726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.manifold._t_sne\")\n",
    "\n",
    "import time\n",
    "\n",
    "import itertools\n",
    "#import torch\n",
    "#from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, random_split, Dataset\n",
    "#from torch.utils.data import DataLoader\n",
    "#import numpy as np\n",
    "#import itertools\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfed4927-1af3-439c-a382-72f28f1c4948",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"natural_movies_2025_07_17/digits/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddf5f92-8b27-4269-9e04-d4dbbebcbef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file path and variable name (adjust as needed)\n",
    "data_name = \"triplet_digits_2\"\n",
    "mat_path = file_path + data_name + \".mat\"\n",
    "#mat_path = file_path + \"allPatches.mat\"\n",
    "#mat_key = \"allPatches\"\n",
    "mat_key = \"triplets\"\n",
    "mat_lab = \"labels\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3ebace",
   "metadata": {},
   "outputs": [],
   "source": [
    "v7 = 1\n",
    "\n",
    "if v7 == 1:\n",
    "    # --- Load from MATLAB v7 (non-HDF5) ---\n",
    "    mat = loadmat(mat_path)  # mat_path is your .mat file\n",
    "    raw = mat[mat_key]       # mat_key is the variable name inside the .mat file\n",
    "    print(\"Raw MAT shape:\", raw.shape)\n",
    "    data = raw  # no transpose if shape is already correct\n",
    "    \n",
    "    labels = mat[\"labels\"]\n",
    "else:\n",
    "    # Load from HDF5\n",
    "    with h5py.File(mat_path, 'r') as f:\n",
    "        raw = f[mat_key]\n",
    "        print(\"Raw HDF5 shape:\", raw.shape)\n",
    "        data = np.array(raw)  # this ensures a clean ndarray\n",
    "        \n",
    "print(data.shape)\n",
    "\n",
    "# Transpose\n",
    "data = data.transpose(3, 2, 0, 1)\n",
    "    \n",
    "# --- Convert to float32 Torch tensor ---\n",
    "triplet_data = torch.tensor(data).float() / 255.0\n",
    "\n",
    "# Compute dataset mean & std once:\n",
    "mean = triplet_data.mean()\n",
    "std = triplet_data.std()\n",
    "\n",
    "# Normalize entire dataset:\n",
    "triplet_data = (triplet_data - mean) / std\n",
    "\n",
    "print(\"Triplet_data shape:\", triplet_data.shape)\n",
    "print(\"Labels:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075a23cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba249475",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(3):\n",
    "    plt.subplot(1,3,ii+1)\n",
    "    plt.imshow(triplet_data[0,ii,:,:].detach().cpu().numpy())\n",
    "    if ii == 0:\n",
    "        plt.title(labels[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ac857f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract anchor, positive, and negative from the triplet tensor\n",
    "anchor   = triplet_data[:, 0, :, :]\n",
    "positive = triplet_data[:, 1, :, :]\n",
    "negative = triplet_data[:, 2, :, :]\n",
    "\n",
    "# Flatten images into vectors\n",
    "a = anchor.reshape(anchor.size(0), -1)\n",
    "p = positive.reshape(positive.size(0), -1)\n",
    "n = negative.reshape(negative.size(0), -1)\n",
    "\n",
    "# Compute distances\n",
    "ap_dist = F.pairwise_distance(a, p)\n",
    "an_dist = F.pairwise_distance(a, n)\n",
    "\n",
    "# Compute margin violations\n",
    "margin = 4.\n",
    "violations = (ap_dist + margin > an_dist).float()\n",
    "\n",
    "# Compute stats\n",
    "mean_ap = ap_dist.mean().item()\n",
    "mean_an = an_dist.mean().item()\n",
    "violation_rate = violations.mean().item()\n",
    "\n",
    "# Print results\n",
    "print(\"Triplet Margin Stats:\")\n",
    "print(f\"  Mean anchor-positive distance: {mean_ap:.4f}\")\n",
    "print(f\"  Mean anchor-negative distance: {mean_an:.4f}\")\n",
    "print(f\"  % Triplets violating margin ({margin}): {violation_rate * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9cac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# Assume triplet_data is a tensor of shape (N, 3, H, W)\n",
    "# Convert to numpy if needed\n",
    "X_np = triplet_data.numpy() if isinstance(triplet_data, torch.Tensor) else triplet_data\n",
    "\n",
    "# Extract positives and negatives\n",
    "positives = X_np[:, 1].reshape(X_np.shape[0], -1)  # shape: (N, H*W)\n",
    "negatives = X_np[:, 2].reshape(X_np.shape[0], -1)\n",
    "\n",
    "# Stack data and labels\n",
    "X = np.vstack([positives, negatives])  # shape: (2N, H*W)\n",
    "y = np.hstack([np.ones(len(positives)), np.zeros(len(negatives))])  # shape: (2N,)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train logistic regression\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "y_score = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_score)\n",
    "\n",
    "print(f\"üîç Logistic Regression Accuracy: {acc:.4f}\")\n",
    "print(f\"üìà ROC AUC Score: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672de0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_index(x):\n",
    "    x = np.array(x, dtype=np.float64)\n",
    "    if np.amin(x) < 0:\n",
    "        raise ValueError(\"Gini index is only defined for non-negative values\")\n",
    "    if np.all(x == 0):\n",
    "        return 0.0  # Convention: Gini is 0 for uniform zero vector\n",
    "\n",
    "    x_sorted = np.sort(x)\n",
    "    n = len(x)\n",
    "    index = np.arange(1, n + 1)\n",
    "    return (2 * np.sum(index * x_sorted) / np.sum(x)) / n - (n + 1) / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7513448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_triplet_margin_stats(model, dataloader, device, margin=0.2):\n",
    "\n",
    "    model.eval()\n",
    "    ap_dists = []\n",
    "    an_dists = []\n",
    "    violations = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for a, p, n in dataloader:\n",
    "            a, p, n = a.to(device), p.to(device), n.to(device)\n",
    "\n",
    "            anchor = model(a)\n",
    "            positive = model(p)\n",
    "            negative = model(n)\n",
    "\n",
    "            ap = F.pairwise_distance(anchor, positive)\n",
    "            an = F.pairwise_distance(anchor, negative)\n",
    "\n",
    "            ap_dists.append(ap)\n",
    "            an_dists.append(an)\n",
    "            violations.append((ap + margin > an).float())\n",
    "\n",
    "    ap_dists = torch.cat(ap_dists)\n",
    "    an_dists = torch.cat(an_dists)\n",
    "    violations = torch.cat(violations)\n",
    "\n",
    "    mean_ap = ap_dists.mean().item()\n",
    "    mean_an = an_dists.mean().item()\n",
    "    violation_rate = violations.mean().item()\n",
    "\n",
    "    return mean_ap, mean_an, violation_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7befba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch.utils.data import Dataset\n",
    "\n",
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, triplet_tensor):\n",
    "        self.triplets = triplet_tensor  # could be a Tensor or a Subset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        triplet = self.triplets[idx]\n",
    "        anchor = triplet[0].unsqueeze(0)  # shape: (1, 16, 16)\n",
    "        positive = triplet[1].unsqueeze(0)\n",
    "        negative = triplet[2].unsqueeze(0)\n",
    "        return anchor, positive, negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca09d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Example dataset with data and labels ----\n",
    "# triplet_data: Tensor of shape [N, 3, 16, 16]\n",
    "# labels:       Numpy array of length N (integer labels)\n",
    "N = len(triplet_data)\n",
    "labels = np.array(labels[0,:])  # Make sure you have labels aligned with triplet_data\n",
    "\n",
    "# ---- Create random permutation of indices ----\n",
    "indices = torch.randperm(N)\n",
    "train_size = int(0.7 * N)\n",
    "val_size = int(0.1 * N)\n",
    "test_size = N - train_size - val_size\n",
    "\n",
    "train_idx = indices[:train_size]\n",
    "val_idx = indices[train_size:train_size + val_size]\n",
    "test_idx = indices[train_size + val_size:]\n",
    "\n",
    "# ---- Split dataset ----\n",
    "train_data = Subset(triplet_data, train_idx)\n",
    "val_data = Subset(triplet_data, val_idx)\n",
    "test_data = Subset(triplet_data, test_idx)\n",
    "\n",
    "# ---- Save label splits ----\n",
    "train_labels = labels[train_idx.numpy()]\n",
    "val_labels = labels[val_idx.numpy()]\n",
    "test_labels = labels[test_idx.numpy()]\n",
    "\n",
    "# ---- Create DataLoaders ----\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(TripletDataset(train_data), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TripletDataset(val_data), batch_size=batch_size)\n",
    "test_loader = DataLoader(TripletDataset(test_data), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f614db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self, embedding_dim=32):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(16*16, embedding_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        out = self.relu(self.fc(x))  # Linear followed by ReLU\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d3eac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loss(model, data_loader, criterion, device, l1_lambda=0.0):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_triplet = 0.0\n",
    "    total_l1 = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for a, p, n in data_loader:\n",
    "            a, p, n = a.to(device), p.to(device), n.to(device)\n",
    "            a_out, p_out, n_out = model(a), model(p), model(n)\n",
    "\n",
    "            triplet_loss = criterion(a_out, p_out, n_out)\n",
    "\n",
    "            l1_penalty = 0.0\n",
    "            if l1_lambda > 0:\n",
    "                l1_penalty = (\n",
    "                    a_out.abs().sum() +\n",
    "                    p_out.abs().sum() +\n",
    "                    n_out.abs().sum()\n",
    "                ) / a_out.shape[0]\n",
    "\n",
    "            loss = triplet_loss + l1_lambda * l1_penalty\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_triplet += triplet_loss.item()\n",
    "            total_l1 += l1_penalty\n",
    "\n",
    "    n_batches = len(data_loader)\n",
    "    return (\n",
    "        total_loss / n_batches,\n",
    "        total_triplet / n_batches,\n",
    "        total_l1 / n_batches\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25491a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel2d(kernel_size=5, sigma=0.25, device='cpu'):\n",
    "    ax = torch.arange(kernel_size, device=device) - kernel_size // 2\n",
    "    xx, yy = torch.meshgrid(ax, ax, indexing=\"xy\")\n",
    "    kernel = torch.exp(-(xx**2 + yy**2) / (2 * sigma**2))\n",
    "    kernel /= kernel.sum()\n",
    "    return kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84af563a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_weights(model, sigma=0.25):\n",
    "    # Get weights and reshape (embedding_dim, 256) -> (embedding_dim, 1, 16, 16)\n",
    "    weight = model.fc.weight.data\n",
    "    emb_dim = weight.shape[0]\n",
    "    weight_reshaped = weight.view(emb_dim, 1, 16, 16)\n",
    "\n",
    "    # Build Gaussian kernel\n",
    "    kernel_size = int(2 * round(3 * sigma) + 1)\n",
    "    kernel = gaussian_kernel2d(kernel_size, sigma, device=weight.device)\n",
    "    kernel = kernel.view(1, 1, kernel_size, kernel_size)\n",
    "\n",
    "    # Convolve all embeddings in one pass (treat each as separate batch)\n",
    "    smoothed = F.conv2d(weight_reshaped, kernel, padding=kernel_size // 2)\n",
    "\n",
    "    # Flatten back and copy into model\n",
    "    model.fc.weight.data.copy_(smoothed.view(emb_dim, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130d8203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, optimizer, criterion, device, data_name,\n",
    "          embedding_dim, epochs=10, patience=5, min_delta=1e-4, l1_lambda=0.0,\n",
    "         margin=0.2):\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    train_triplet_losses, val_triplet_losses = [], []\n",
    "    train_l1_norms, val_l1_norms = [], []\n",
    "    \n",
    "    train_violations, val_violations = [], []\n",
    "    \n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_state = None\n",
    "    best_epoch = 0\n",
    "    wait = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    # ---- Initial evaluation before training ----\n",
    "    train_loss, train_triplet_loss, train_l1_norm = evaluate_loss(model, train_loader,\n",
    "                                                            criterion, device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_triplet_losses.append(train_triplet_loss)\n",
    "    train_l1_norms.append(train_l1_norm)\n",
    "    \n",
    "    _, _, v_train = compute_triplet_margin_stats(model, train_loader, device)\n",
    "    train_violations.append(v_train)\n",
    "\n",
    "    val_loss, val_triplet_loss, val_l1_norm = evaluate_loss(model, val_loader,\n",
    "                                                            criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_triplet_losses.append(val_triplet_loss)\n",
    "    val_l1_norms.append(val_l1_norm)\n",
    "    \n",
    "    _, _, v_val = compute_triplet_margin_stats(model, val_loader, device)\n",
    "    val_violations.append(v_val)\n",
    "\n",
    "    best_val_loss = val_losses[-1]\n",
    "    best_state = model.state_dict()\n",
    "\n",
    "    # ---- Training loop ----\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_triplet = 0.0\n",
    "        total_l1 = 0.0\n",
    "        \n",
    "        for a, p, n in train_loader:\n",
    "            a, p, n = a.to(device), p.to(device), n.to(device)\n",
    "            a_out, p_out, n_out = model(a), model(p), model(n)\n",
    "            triplet_loss = criterion(a_out, p_out, n_out)\n",
    "\n",
    "            l1_penalty = 0.0\n",
    "            if l1_lambda > 0:\n",
    "                l1_penalty = (\n",
    "                    a_out.abs().sum() +\n",
    "                    p_out.abs().sum() +\n",
    "                    n_out.abs().sum()\n",
    "                ) / a_out.shape[0]\n",
    "\n",
    "            loss = triplet_loss + l1_lambda * l1_penalty\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_triplet += triplet_loss.item()\n",
    "            total_l1 += l1_penalty\n",
    "\n",
    "        \n",
    "            \n",
    "        train_losses.append(total_loss / len(train_loader))\n",
    "        train_triplet_losses.append(total_triplet / len(train_loader))\n",
    "        train_l1_norms.append(total_l1 / len(train_loader))\n",
    "        \n",
    "        smooth_weights(model, sigma=0.25)\n",
    "        \n",
    "        # compute violations\n",
    "        _, _, v_train = compute_triplet_margin_stats(model, train_loader, device)\n",
    "        train_violations.append(v_train)\n",
    "\n",
    "        val_loss, val_triplet_loss, val_l1_norm = evaluate_loss(model, val_loader, criterion, device, l1_lambda)\n",
    "        val_losses.append(val_loss)\n",
    "        val_triplet_losses.append(val_triplet_loss)\n",
    "        val_l1_norms.append(val_l1_norm)\n",
    "        _, _, v_val = compute_triplet_margin_stats(model, val_loader, device)\n",
    "        val_violations.append(v_val)\n",
    "\n",
    "        if val_loss < best_val_loss - min_delta:\n",
    "            best_val_loss = val_loss\n",
    "            best_state = model.state_dict()\n",
    "            best_epoch = epoch + 1\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"\\n‚èπÔ∏è Early stopping at epoch {epoch + 1}\")\n",
    "                break\n",
    "\n",
    "        print('.', end='', flush=True)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f' Epoch {epoch + 1}')\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTraining completed in {total_time:.2f} seconds\")\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "\n",
    "        \n",
    "    # Save model state separately\n",
    "    #lambda_str = \"0\" if l1_lambda == 0 else f\"{l1_lambda:.0e}\".replace('-', 'm')\n",
    "    margin_str = f\"{margin:.1f}\".replace('.', 'p')\n",
    "    model_path = f\"models/class_{data_name}_M{margin_str}_ED{embedding_dim}_EP{epochs}.pt\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    # Save metadata separately\n",
    "    meta_path = f\"models/class_{data_name}_M{margin_str}_ED{embedding_dim}_EP{epochs}.pkl\"\n",
    "    with open(meta_path, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            \"train_losses\": train_losses,\n",
    "            \"val_losses\": val_losses,\n",
    "            \"train_triplet_losses\": train_triplet_losses,\n",
    "            \"val_triplet_losses\": val_triplet_losses,\n",
    "            \"train_l1_norms\": train_l1_norms,\n",
    "            \"val_l1_norms\": val_l1_norms,\n",
    "            \"train_viol\": train_violations,\n",
    "            \"val_viol\": val_violations,\n",
    "            \"train_time\": total_time,\n",
    "            \"best_epoch\": best_epoch\n",
    "        }, f)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d41160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed hyperparameters\n",
    "epochs = 300\n",
    "patience = 5\n",
    "min_delta = 1e-4\n",
    "\n",
    "# --- Hyperparameter grid ---\n",
    "learning_rate = [1e-4]\n",
    "weight_decays = [0]\n",
    "batch_sizes = [64]\n",
    "\n",
    "l1_lambdas = [0]\n",
    "# embedding_dims = [8, 16, 32, 64, 128, 256, 512]\n",
    "# margins = [0.1, 0.2, 0.3, 0.4]\n",
    "\n",
    "embedding_dims = [32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]\n",
    "margins = [3., 4., 5.]\n",
    "\n",
    "hyper_params = []\n",
    "model_id = 0\n",
    "\n",
    "\n",
    "for lr, wd, bs, ed, ll, mm in itertools.product(learning_rate, weight_decays, batch_sizes,\\\n",
    "                                            embedding_dims, l1_lambdas, margins):\n",
    "    hyper_params.append({\n",
    "        \"model id\": model_id,\n",
    "        \"learning rate\": lr,\n",
    "        \"weight decay\": wd,\n",
    "        \"batch size\": bs,\n",
    "        \"embedding dim\": ed,\n",
    "        \"l1 lambda\": ll,\n",
    "        \"margin\": mm,\n",
    "        \"epochs\": epochs,\n",
    "        \"patience\": patience,\n",
    "        \"min delta\": min_delta,\n",
    "    })\n",
    "\n",
    "    \n",
    "n_models = len(hyper_params)\n",
    "print(n_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1ffe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_id in range(n_models):\n",
    "    lr = hyper_params[model_id][\"learning rate\"]\n",
    "    wd = hyper_params[model_id][\"weight decay\"]\n",
    "    bs = hyper_params[model_id][\"batch size\"]\n",
    "    ed = hyper_params[model_id][\"embedding dim\"]\n",
    "    ll = hyper_params[model_id][\"l1 lambda\"]\n",
    "    epochs = hyper_params[model_id][\"epochs\"]\n",
    "    margin = hyper_params[model_id][\"margin\"]\n",
    "    patience = hyper_params[model_id][\"patience\"]\n",
    "    min_delta = hyper_params[model_id][\"min delta\"]\n",
    "    #lambda_str = \"0\" if ll == 0 else f\"{ll:.0e}\".replace('-', 'm')\n",
    "    margin_str = f\"{margin:.1f}\".replace('.', 'p')\n",
    "    hyper_path = f\"models/class_{data_name}_hyper_M{margin_str}_ED{ed}_EP{epochs}.pkl\"\n",
    "    with open(hyper_path, 'wb') as f:\n",
    "        pickle.dump({\n",
    "        \"model id\": model_id,\n",
    "        \"learning rate\": lr,\n",
    "        \"weight decay\": wd,\n",
    "        \"batch size\": bs,\n",
    "        \"embedding dim\": ed,\n",
    "        \"l1 lambda\": ll,\n",
    "        \"epochs\": epochs,\n",
    "        \"margin\": margin,\n",
    "        \"patience\": patience,\n",
    "        \"min delta\": min_delta,\n",
    "        }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e884e039",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Iterate over all combinations ---\n",
    "for model_id in range(n_models):\n",
    "    learning_rate = hyper_params[model_id][\"learning rate\"]\n",
    "    wd = hyper_params[model_id][\"weight decay\"]\n",
    "    bs = hyper_params[model_id][\"batch size\"]\n",
    "    ed = hyper_params[model_id][\"embedding dim\"]\n",
    "    ll = hyper_params[model_id][\"l1 lambda\"]\n",
    "    epochs = hyper_params[model_id][\"epochs\"]\n",
    "    margin = hyper_params[model_id][\"margin\"]\n",
    "    patience = hyper_params[model_id][\"patience\"]\n",
    "    min_delta = hyper_params[model_id][\"min delta\"]\n",
    "    \n",
    "    #lambda_str = \"0\" if ll == 0 else f\"{ll:.0e}\".replace('-', 'm')\n",
    "    #margin_str = f\"{margin:.1f}\".replace('.', 'p')\n",
    "    print(f\"\\nTraining {model_id+1}/{n_models} with M={margin}, ED={ed}, WD={wd}, BS={bs}\")\n",
    "    # Dataloaders\n",
    "    train_loader = DataLoader(TripletDataset(train_data), batch_size=bs, shuffle=True)\n",
    "    val_loader = DataLoader(TripletDataset(val_data), batch_size=bs)\n",
    "\n",
    "    model = EmbeddingNet(embedding_dim=ed)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=wd)\n",
    "    criterion = nn.TripletMarginLoss(margin=margin)\n",
    "\n",
    "    model = train(model,train_loader,val_loader,\n",
    "                    optimizer,criterion,device,\n",
    "                    data_name, embedding_dim=ed, epochs=epochs,patience=patience,\n",
    "                      min_delta=min_delta, l1_lambda=ll,margin=margin)\n",
    "\n",
    "print(f\"Grid completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e56d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for hp in hyper_params:  # assuming hyper_params is a list of dicts with 'hidden dim' and 'embedding dim'\n",
    "    embedding_dim = hp['embedding dim']\n",
    "    margin = hp['margin']\n",
    "    #lambda_str = \"0\" if l1_lambda == 0 else f\"{l1_lambda:.0e}\".replace('-', 'm')\n",
    "    margin_str = f\"{margin:.1f}\".replace('.', 'p')\n",
    "    filename = f\"models/class_{data_name}_M{margin_str}_ED{embedding_dim}_EP{epochs}.pkl\"\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"‚ö†Ô∏è File not found: {filename}\")\n",
    "        continue\n",
    "\n",
    "    with open(filename, 'rb') as f:\n",
    "        print(filename)\n",
    "        saved = pickle.load(f)\n",
    "\n",
    "    results.append({\n",
    "        \"train_losses\": saved[\"train_losses\"],\n",
    "        \"val_losses\": saved[\"val_losses\"],\n",
    "        \"train_viol\": saved[\"train_viol\"],\n",
    "        \"val_viol\": saved[\"val_viol\"],\n",
    "        \"train_time\": saved[\"train_time\"],\n",
    "        \"best_epoch\": saved[\"best_epoch\"],        \n",
    "    })\n",
    "\n",
    "print(f\" Loaded {len(results)} models from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98255fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Get all unique margins ---\n",
    "margins = sorted(set(hp[\"margin\"] for hp in hyper_params))\n",
    "\n",
    "best_m_id = []\n",
    "\n",
    "print(\"\\n Best model for each margin:\")\n",
    "for m in margins:\n",
    "    # All models with this margin\n",
    "    margin_ids = [i for i, hp in enumerate(hyper_params) if hp[\"margin\"] == m]\n",
    "    \n",
    "    # Pick the one with minimum validation loss\n",
    "    best_id = min(margin_ids, key=lambda i: min(results[i][\"val_losses\"]))\n",
    "    best_m_id.append(best_id)\n",
    "    \n",
    "    # Print summary for this margin\n",
    "    best_loss = min(results[best_id][\"val_losses\"])\n",
    "    ed = hyper_params[best_id]['embedding dim']\n",
    "    print(f\"Margin={m}: Model ID={best_id}, ED={ed}, Best Validation Loss={best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84c8a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sort and pick best based on early stopping ---\n",
    "best_id = min(range(len(results)), key=lambda i: min(results[i][\"val_losses\"]))\n",
    "best_result = results[best_id]\n",
    "best_hyperparams = hyper_params[best_id]\n",
    "best_val_loss = min(best_result[\"val_losses\"])\n",
    "best_val_viol = min(best_result[\"val_viol\"])\n",
    "\n",
    "print(\"\\n Best config:\")\n",
    "print(f\"   ED={best_hyperparams['embedding dim']}, \"\n",
    "    f\"M={best_hyperparams['margin']}\")\n",
    "print(f\"   Best Validation Loss={best_val_loss:.4f}\")\n",
    "print(f\"   Training Time: {best_result['train_time']:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b80cd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_output_activations(model, model_file, test_loader, device):\n",
    "\n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    out_acts = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for anchor, positive, negative in test_loader:\n",
    "            anchor = anchor.to(device)\n",
    "            positive = positive.to(device)\n",
    "            negative = negative.to(device)\n",
    "\n",
    "            out_anchor = model(anchor)\n",
    "            out_positive = model(positive)\n",
    "            out_negative = model(negative)\n",
    "\n",
    "            out_batch = torch.stack([out_anchor, out_positive, out_negative], dim=1)\n",
    "            out_acts.append(out_batch.cpu())\n",
    "\n",
    "    out_acts = torch.cat(out_acts, dim=0).numpy()\n",
    "    return out_acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98caf4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 20000\n",
    "test_results = []\n",
    "emb_act = []\n",
    "for model_id in range(n_models):\n",
    "    ed = hyper_params[model_id][\"embedding dim\"]\n",
    "    margin = hyper_params[model_id][\"margin\"]\n",
    "    margin_str = f\"{margin:.1f}\".replace('.', 'p')\n",
    "    model_file = f\"models/class_{data_name}_M{margin_str}_ED{ed}_EP{epochs}.pt\"\n",
    "    print(f\"M{margin_str}, ED{ed}, EP{epochs}\")\n",
    "    if not os.path.exists(model_file):\n",
    "        print(f\" Missing files for ED={ed}\")\n",
    "        continue\n",
    "\n",
    "    model = EmbeddingNet(embedding_dim=ed)\n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    criterion = nn.TripletMarginLoss(margin=margin)\n",
    "    # --- Compute test loss using evaluate_loss ---\n",
    "    test_loss, test_triplet_loss, test_l1_norm = evaluate_loss(model, test_loader,\n",
    "                                                                  criterion, device)\n",
    "\n",
    "    # --- Compute margin violation percentage on test set ---\n",
    "    _, _, test_violation_pct = compute_triplet_margin_stats(model, test_loader,\n",
    "                                                            device, margin=0.2)\n",
    "\n",
    "    # Compute Gini index\n",
    "    out_acts = extract_output_activations(model=model,model_file=model_file,\n",
    "        test_loader=test_loader,device=device)\n",
    "    #print(\"Hidden activations shape:\", out_acts.shape)\n",
    "    emb_act.append(out_acts[:,0,:])\n",
    "    gini_act = np.zeros((n_test,3))\n",
    "    for i in range(n_test):\n",
    "        for j in range(3):\n",
    "            gini_act[i,j] = gini_index(out_acts[i,j,:])\n",
    "    gini_act_m = np.mean(gini_act)\n",
    "    \n",
    "    test_results.append({\n",
    "        \"test_losses\": test_loss,\n",
    "        \"test_triplet_loss\": test_triplet_loss,\n",
    "        \"test_l1_norm\": test_l1_norm,\n",
    "        \"test_viol\": test_violation_pct,\n",
    "        \"test_gini\": gini_act_m,\n",
    "    })\n",
    "    \n",
    "    test_path = f\"models/class_test_{data_name}_M{margin_str}_ED{ed}_EP{epochs}.pkl\"\n",
    "    with open(test_path, 'wb') as f:\n",
    "        pickle.dump({\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_triplet_loss\": test_triplet_loss,\n",
    "        \"test_l1_norm\": test_l1_norm,\n",
    "        \"test_viol\": test_violation_pct,\n",
    "        \"test_gini\": gini_act_m,\n",
    "        }, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2f437f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_param(val):\n",
    "    if val == 0:\n",
    "        return \"0\"\n",
    "    elif val < 1:\n",
    "        return f\"{val:.0e}\"\n",
    "    else:\n",
    "        return str(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0a1e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min(result['best_epoch']  for result in results))\n",
    "print(max(result['best_epoch']  for result in results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4f176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"losses\"\n",
    "metric = \"viol\"\n",
    "\n",
    "min_loss = min(min(r[\"train_\"+metric] + r[\"val_\"+metric]) for r in results)\n",
    "max_loss = max(max(r[\"train_\"+metric] + r[\"val_\"+metric]) for r in results)\n",
    "\n",
    "xvar = 'embedding dim'\n",
    "yvar = 'margin'\n",
    "xvar2 = 'ED'\n",
    "yvar2 = 'M'\n",
    "xvals = sorted(set(r[xvar] for r in hyper_params))\n",
    "yvals = sorted(set(r[yvar] for r in hyper_params))\n",
    "nx = len(xvals)\n",
    "ny = len(yvals)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(nx*2,ny*2))\n",
    "ip = 0\n",
    "\n",
    "for yval in yvals:\n",
    "    \n",
    "\n",
    "    #ed = hyper_params[model_id][\"embedding dim\"]\n",
    "    for xval in xvals:\n",
    "\n",
    "        ip += 1\n",
    "        plt.subplot(ny,nx,ip)\n",
    "        \n",
    "        #xval = hyper_params[model_id][\"margin\"]\n",
    "        model_id = next(i for i, r in enumerate(hyper_params)\n",
    "                            if r[xvar] == xval and r[yvar] == yval)\n",
    "\n",
    "\n",
    "        hyper_p = f\"{yvar2}={yval}, {xvar2}={xval}\"\n",
    "\n",
    "        best_id = best_m_id[yvals.index(yval)]\n",
    "        plt.plot(results[best_id]['train_'+metric],'k--',alpha=0.5)\n",
    "        plt.plot(results[best_id]['val_'+metric],'k-',alpha=0.5)\n",
    "        plt.plot(results[best_id]['best_epoch'],test_results[best_id]['test_'+metric],'ko',alpha=0.5)\n",
    "\n",
    "        plt.plot(results[model_id]['train_'+metric],'b--')\n",
    "        plt.plot(results[model_id]['val_'+metric],'b-')\n",
    "        plt.plot(results[model_id]['best_epoch'],test_results[model_id]['test_'+metric],'bo')\n",
    "        plt.ylim(min_loss,max_loss)\n",
    "        plt.title(hyper_p)\n",
    "        #plt.yscale('log')\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeda2cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_i = [\"viol\",\"losses\",\"gini\"]\n",
    "#metric = \"viol\"\n",
    "#metric = \"losses\"\n",
    "#metric = \"gini\"\n",
    "mouse_exp = 20*16*16\n",
    "human_exp = 300*16*16\n",
    "\n",
    "xvar = 'embedding dim'\n",
    "yvar = 'margin'\n",
    "xvals = sorted(set(r[xvar] for r in hyper_params))\n",
    "yvals = sorted(set(r[yvar] for r in hyper_params))\n",
    "\n",
    "# Define styles\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple']\n",
    "yvals_to_color = {ed: colors[i % len(colors)] for i, ed in enumerate(yvals)}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "ip = 0\n",
    "for metric in metric_i:\n",
    "    \n",
    "    \n",
    "    xval_array = np.array(xvals)\n",
    "\n",
    "    for yval in [2.0]:\n",
    "        ip += 1\n",
    "        plt.subplot(3,5,ip)\n",
    "\n",
    "        test_losses = []\n",
    "        for xval in xvals:\n",
    "            model_id = next(i for i, r in enumerate(hyper_params)\n",
    "                            if r[xvar] == xval and r[yvar] == yval)\n",
    "\n",
    "            test_losses.append(test_results[model_id]['test_'+metric])\n",
    "        print(min(test_losses))\n",
    "        color = yvals_to_color[yval]\n",
    "        label = f\"{yvar}={yval}\"\n",
    "        plt.plot(xval_array, test_losses, 'o-',label=label,alpha=0.5)\n",
    "        yl = plt.ylim()  # get current y-axis limits\n",
    "        #plt.plot(mouse_exp*np.ones(2), yl, '--r', linewidth=2)  # vertical line at x=3\n",
    "        #plt.plot(human_exp*np.ones(2), yl, '-.g', linewidth=2)  # vertical line at x=7\n",
    "        # Formatting\n",
    "        plt.xscale(\"log\")\n",
    "        plt.xlabel(\"Embedding dimension\")\n",
    "        if metric == \"viol\":\n",
    "            plt.ylabel(\"Fraction violations\")\n",
    "        elif metric == \"losses\":\n",
    "            plt.ylabel(\"Triplet loss\")\n",
    "        elif metric == \"gini\":\n",
    "            plt.ylabel(\"Gini index\")\n",
    "\n",
    "        \n",
    "        #plt.title(label)\n",
    "        #plt.legend()\n",
    "        plt.tight_layout()\n",
    "plt.savefig('figures/test_results_margin4.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07c357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_i = [\"viol\",\"losses\",\"gini\"]\n",
    "#metric = \"viol\"\n",
    "#metric = \"losses\"\n",
    "#metric = \"gini\"\n",
    "mouse_exp = 20*16*16\n",
    "human_exp = 300*16*16\n",
    "\n",
    "xvar = 'embedding dim'\n",
    "yvar = 'margin'\n",
    "xvals = sorted(set(r[xvar] for r in hyper_params))\n",
    "yvals = sorted(set(r[yvar] for r in hyper_params))\n",
    "nx = len(xvals)\n",
    "ny = len(yvals)\n",
    "nm = len(metric_i)\n",
    "\n",
    "# Define styles\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple']\n",
    "yvals_to_color = {ed: colors[i % len(colors)] for i, ed in enumerate(yvals)}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(ny*2, nm*2))\n",
    "ip = 0\n",
    "for metric in metric_i:\n",
    "    \n",
    "    \n",
    "    xval_array = np.array(xvals)\n",
    "\n",
    "    for yval in yvals:\n",
    "        ip += 1\n",
    "        plt.subplot(nm,ny,ip)\n",
    "\n",
    "        test_losses = []\n",
    "        for xval in xvals:\n",
    "            model_id = next(i for i, r in enumerate(hyper_params)\n",
    "                            if r[xvar] == xval and r[yvar] == yval)\n",
    "\n",
    "            test_losses.append(test_results[model_id]['test_'+metric])\n",
    "\n",
    "        color = yvals_to_color[yval]\n",
    "        label = f\"{yvar}={yval}\"\n",
    "        plt.plot(xval_array, test_losses, 'o-',label=label,alpha=0.5)\n",
    "        yl = plt.ylim()  # get current y-axis limits\n",
    "        #plt.plot(mouse_exp*np.ones(2), yl, '--r', linewidth=2)  # vertical line at x=3\n",
    "        \n",
    "        # Formatting\n",
    "        plt.xscale(\"log\")\n",
    "        plt.xlabel(xvar)\n",
    "        plt.ylabel(metric)\n",
    "        plt.title(label)\n",
    "        #plt.legend()\n",
    "        plt.tight_layout()\n",
    "plt.savefig(f'figures/test_results_{data_name}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea97edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange_blocks_fixed(blocks, p, q):\n",
    "    m, n1, n2 = blocks.shape\n",
    "    assert n1 == n2, \"Blocks must be square\"\n",
    "    assert m == p * q, \"m must equal p * q\"\n",
    "    n = n1\n",
    "    reshaped = blocks.reshape(p, q, n, n)\n",
    "    merged = reshaped.swapaxes(1, 2).reshape(p * n, q * n)\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b1f581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resmooth_weights(model, sigma=0.25):\n",
    "    weight = model.fc.weight.data\n",
    "    emb_dim = weight.shape[0]\n",
    "    weight_reshaped = weight.view(emb_dim, 1, 16, 16)\n",
    "\n",
    "    kernel_size = int(2 * round(3 * sigma) + 1)\n",
    "    kernel = gaussian_kernel2d(kernel_size, sigma, device=weight.device)\n",
    "    kernel = kernel.view(1, 1, kernel_size, kernel_size)\n",
    "\n",
    "    smoothed = F.conv2d(weight_reshaped, kernel, padding=kernel_size // 2)\n",
    "    model.fc.weight.data.copy_(smoothed.view(emb_dim, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de3918d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "weights_reshaped = []\n",
    "for model_id in range(n_models):\n",
    "\n",
    "    ed = hyper_params[model_id][\"embedding dim\"]\n",
    "\n",
    "    margin = hyper_params[model_id][\"margin\"]\n",
    "    margin_str = f\"{margin:.1f}\".replace('.', 'p')\n",
    "    model_file = f\"models/class_{data_name}_M{margin_str}_ED{ed}_EP{epochs}.pt\"\n",
    "    model = EmbeddingNet(embedding_dim=ed)\n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # ---- Smooth the weights again ----\n",
    "    resmooth_weights(model, sigma=.5)\n",
    "\n",
    "    # Extract weights from first layer (input to hidden): shape (n_hidden_units, 256)\n",
    "    first_layer = model.fc  # Replace if you used a different name\n",
    "    weights = first_layer.weight.detach().cpu().numpy()  # shape: (n_hidden_units, 256)\n",
    "\n",
    "    # Reshape to (n_hidden_units, 16, 16)\n",
    "    weights_reshaped.append(weights.reshape(weights.shape[0], 16, 16))\n",
    "\n",
    "    #print(\"Weight shape:\", weights_reshaped[model_id].shape)  # (n_hidden_units, 16, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b28dabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "n = 16\n",
    "p = 4 # rows \n",
    "q = 8 # columns\n",
    "size_factor = 1.\n",
    "for model_id in range(n_models):\n",
    "    \n",
    "    embedding_dim = hyper_params[model_id]['embedding dim']\n",
    "    margin = hyper_params[model_id]['margin']\n",
    "    # ---- Filter ----\n",
    "    if margin != 3.0 or embedding_dim not in [32, 1024]:\n",
    "        continue  # skip models that don't match criteria\n",
    "    \n",
    "    merged_rf = rearrange_blocks_fixed(weights_reshaped[model_id][:32,:,:], p=p, q=q)\n",
    "    plt.figure(figsize=(q*size_factor,p*size_factor))\n",
    "    plt.imshow(merged_rf)\n",
    "    for i in range(q+1):\n",
    "        plt.plot(i*n*np.ones(2)-0.5,np.array([0,n*p])-0.5,'w')\n",
    "    for i in range(p+1):\n",
    "        plt.plot(np.array([0,n*q])-0.5,i*n*np.ones(2)-0.5,'w')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    embedding_dim = hyper_params[model_id]['embedding dim']\n",
    "    margin = hyper_params[model_id]['margin']\n",
    "    #lambda_str = \"0\" if l1_lambda == 0 else f\"{l1_lambda:.0e}\".replace('-', 'm')\n",
    "    plt.title(f\"{data_name}, M={margin}, ED={embedding_dim}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    margin_str = f\"{margin:.1f}\".replace('.', 'p')\n",
    "    plt.savefig(f'figures/RF_class_{data_name}_M{margin_str}_ED{embedding_dim}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aed7d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "margin = 3.\n",
    "ed = 1024\n",
    "\n",
    "model_id = next(i for i, r in enumerate(hyper_params)\n",
    "                if r[\"embedding dim\"] == ed and\n",
    "                r[\"margin\"] == margin)\n",
    "print(model_id)\n",
    "print(emb_act[model_id].shape)\n",
    "\n",
    "ed = 32\n",
    "model_id_32 = next(i for i, r in enumerate(hyper_params)\n",
    "                if r[\"embedding dim\"] == ed and\n",
    "                r[\"margin\"] == margin)\n",
    "print(model_id_32)\n",
    "print(emb_act[model_id_32].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8989479",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(triplet_data[indices, 0, :, :].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b116b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract anchors (only channel 0)\n",
    "anchors = triplet_data[indices, 0, :, :]  # shape: (20000, 16, 16)\n",
    "\n",
    "# Flatten to 20000 x 256 safely\n",
    "anchors_flat = anchors.reshape(anchors.size(0), -1)  # or use .contiguous().view(...)\n",
    "\n",
    "# Convert to NumPy\n",
    "anchors_np = anchors_flat.numpy()\n",
    "print(\"Shape:\", anchors_np.shape)  # (20000, 256)\n",
    "\n",
    "np.save(f'models/anchors_data_{data_name}.npy',anchors_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7554205",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = hyper_params[model_id][\"epochs\"]\n",
    "np.savez(f\"models/emb_act_data_{data_name}_EP{epochs}.npz\", **{f\"arr_{i}\": arr for i, arr in enumerate(emb_act)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98071f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.save(f'models/test_labels_{data_name}.npy',test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b6ad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fit PCA ---\n",
    "pca0 = PCA(n_components=2)  # 2D projection\n",
    "pca0.fit(anchors_np)  # shape: (20000, 2)\n",
    "anchors_pca = pca0.transform(anchors_np)  # shape: (20000, 2)\n",
    "\n",
    "# --- Explained variance ---\n",
    "print(\"Explained variance ratio:\", pca0.explained_variance_ratio_)\n",
    "print(\"Total variance explained:\", pca0.explained_variance_ratio_.sum())\n",
    "\n",
    "\n",
    "# --- Fit PCA ---\n",
    "pca32 = PCA(n_components=2)  # 2D projection\n",
    "pca32.fit(emb_act[model_id_32])  # shape: (20000, 2)\n",
    "emb_pca32 = pca32.transform(emb_act[model_id_32])  # shape: (20000, 2)\n",
    "\n",
    "# --- Explained variance ---\n",
    "print(\"Explained variance ratio:\", pca32.explained_variance_ratio_)\n",
    "print(\"Total variance explained:\", pca32.explained_variance_ratio_.sum())\n",
    "\n",
    "# --- Fit PCA ---\n",
    "pca = PCA(n_components=2)  # 2D projection\n",
    "pca.fit(emb_act[model_id])  # shape: (20000, 2)\n",
    "emb_pca = pca.transform(emb_act[model_id])  # shape: (20000, 2)\n",
    "\n",
    "# --- Explained variance ---\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
    "print(\"Total variance explained:\", pca.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ad438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot ---\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1,3,1)\n",
    "scatter = plt.scatter(anchors_pca[:, 0], anchors_pca[:, 1], \n",
    "                      c=test_labels, cmap=\"tab10\", s=2)\n",
    "#plt.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "plt.title(\"PCA, 256 input dimensions\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "scatter = plt.scatter(emb_pca32[:, 0], emb_pca32[:, 1], \n",
    "                      c=test_labels, cmap=\"tab10\", s=2)\n",
    "#plt.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "plt.title(\"PCA, 32 embedding dimensions\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "scatter = plt.scatter(emb_pca[:, 0], emb_pca[:, 1], \n",
    "                      c=test_labels, cmap=\"tab10\", s=2)\n",
    "#plt.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "plt.title(\"PCA, 1024 embedding dimensions\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f'figures/PCA_{data_name}_M{margin_str}_ED{ed}.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e44be16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dfaaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_split_indices(n_samples, val_size=0.2, test_size=0.2, random_state=42):\n",
    "    # First split train+val and test\n",
    "    train_val_idx, test_idx = train_test_split(np.arange(n_samples),\n",
    "                                               test_size=test_size,\n",
    "                                               random_state=random_state,\n",
    "                                               shuffle=True)\n",
    "    # Split train and val\n",
    "    train_idx, val_idx = train_test_split(train_val_idx,\n",
    "                                          test_size=val_size/(1-test_size),\n",
    "                                          random_state=random_state,\n",
    "                                          shuffle=True)\n",
    "    return train_idx, val_idx, test_idx\n",
    "\n",
    "# Example usage:\n",
    "N = len(labels)  # number of samples\n",
    "train_idx, val_idx, test_idx = create_split_indices(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f81612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_softmax_classifier_pre_split(X_train, y_train, X_val, y_val,\n",
    "                                       lr=1e-3, weight_decay=1e-3,\n",
    "                                       epochs=2000, patience=20):\n",
    "    \"\"\"\n",
    "    Train a linear softmax classifier using pre-split data.\n",
    "    \"\"\"\n",
    "    # ---- Ensure tensors and correct dtypes ----\n",
    "    if not isinstance(X_train, torch.Tensor):\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    else:\n",
    "        X_train = X_train.float()\n",
    "\n",
    "    if not isinstance(X_val, torch.Tensor):\n",
    "        X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "    else:\n",
    "        X_val = X_val.float()\n",
    "\n",
    "    if not isinstance(y_train, torch.Tensor):\n",
    "        y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "    else:\n",
    "        y_train = y_train.long()\n",
    "\n",
    "    if not isinstance(y_val, torch.Tensor):\n",
    "        y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "    else:\n",
    "        y_val = y_val.long()\n",
    "\n",
    "    # ---- Model, Loss, Optimizer ----\n",
    "    model = LinearSoftmax(X_train.shape[1], len(torch.unique(y_train)))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # ---- Tracking ----\n",
    "    history = {\n",
    "        \"val_loss\": [], \"val_acc\": [],\n",
    "        \"train_loss\": [], \"train_acc\": []\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_state = None\n",
    "    wait = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # --- Train ---\n",
    "        model.train()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss = loss.item()\n",
    "        train_pred = outputs.argmax(dim=1)\n",
    "        train_acc = (train_pred == y_train).float().mean().item()\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs, y_val).item()\n",
    "            val_pred = val_outputs.argmax(dim=1)\n",
    "            val_acc = (val_pred == y_val).float().mean().item()\n",
    "\n",
    "        # ---- Store metrics ----\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        # --- Early stopping ---\n",
    "        if val_loss < best_val_loss - 1e-5:\n",
    "            best_val_loss = val_loss\n",
    "            best_state = model.state_dict()\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a58bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSoftmax(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)  # softmax handled by loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca50d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Compute test loss and accuracy for a trained model.\n",
    "    \"\"\"\n",
    "    # Ensure correct tensor types\n",
    "    if not isinstance(X_test, torch.Tensor):\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    else:\n",
    "        X_test = X_test.float()\n",
    "\n",
    "    if not isinstance(y_test, torch.Tensor):\n",
    "        y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "    else:\n",
    "        y_test = y_test.long()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test)\n",
    "        loss = criterion(outputs, y_test).item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        acc = (preds == y_test).float().mean().item()\n",
    "\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8110ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensors\n",
    "X1 = torch.tensor(anchors_np, dtype=torch.float32)  # shape: (20000, 256)\n",
    "X2a = torch.tensor(emb_act[model_id_32], dtype=torch.float32)  # shape: (20000, 1024)\n",
    "X2 = torch.tensor(emb_act[model_id], dtype=torch.float32)  # shape: (20000, 1024)\n",
    "y = torch.tensor(test_labels, dtype=torch.long)         # shape: (20000,)\n",
    "\n",
    "train_idx, val_idx, test_idx = create_split_indices(n_samples=20000, val_size=0.2,\n",
    "                                                    test_size=0.2, random_state=42)\n",
    "\n",
    "X1_train, X1_val, X1_test = X1[train_idx], X1[val_idx], X1[test_idx]\n",
    "X2a_train, X2a_val, X2a_test = X2a[train_idx], X2a[val_idx], X2a[test_idx]\n",
    "X2_train, X2_val, X2_test = X2[train_idx], X2[val_idx], X2[test_idx]\n",
    "y_train, y_val, y_test = y[train_idx], y[val_idx], y[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98babf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose A and B are PyTorch tensors or datasets\n",
    "len_A, len_B = 12000, 4000\n",
    "\n",
    "# Index vectors for A and B\n",
    "indices_A = torch.arange(len_A)\n",
    "indices_B = torch.arange(len_B)\n",
    "\n",
    "sizes_A = [120,1200,12000]\n",
    "sizes_B = [40,400,4000]\n",
    "\n",
    "model1_i = []\n",
    "model2a_i = []\n",
    "model2_i = []\n",
    "\n",
    "for size_A, size_B in zip(sizes_A, sizes_B):\n",
    "    idx_A = indices_A[:size_A]  # take first size_A samples of A\n",
    "    idx_B = indices_B[:size_B]  # take first size_B samples of B\n",
    "    \n",
    "    # Train on one dataset (e.g., X1)\n",
    "    model1, history1 = train_softmax_classifier_pre_split(X1_train[idx_A], y_train[idx_A],\n",
    "                                                          X1_val[idx_B], y_val[idx_B])\n",
    "    model1_i.append(model1)\n",
    "    print()\n",
    "    \n",
    "    # Train on another dataset (e.g., X2)\n",
    "    model2a, history2 = train_softmax_classifier_pre_split(X2a_train[idx_A], y_train[idx_A],\n",
    "                                                          X2a_val[idx_B], y_val[idx_B])\n",
    "    model2a_i.append(model2a)\n",
    "    print()\n",
    "    \n",
    "    # Train on another dataset (e.g., X2)\n",
    "    model2, history2 = train_softmax_classifier_pre_split(X2_train[idx_A], y_train[idx_A],\n",
    "                                                          X2_val[idx_B], y_val[idx_B])\n",
    "    model2_i.append(model2)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c91752",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc1_i = []\n",
    "test_acc2a_i = []\n",
    "test_acc2_i = []\n",
    "for i in range(3):\n",
    "    test_loss1, test_acc1 = evaluate_model(model1_i[i], X1_test, y_test)\n",
    "    print(f\"Dataset X1 - Test Loss: {test_loss1:.4f}, Test Accuracy: {test_acc1:.4f}\")\n",
    "    test_acc1_i.append(test_acc1)\n",
    "    # Train on another dataset (e.g., X2)\n",
    "    test_loss2a, test_acc2a = evaluate_model(model2a_i[i], X2a_test, y_test)\n",
    "    print(f\"Dataset X2 - Test Loss: {test_loss2a:.4f}, Test Accuracy: {test_acc2a:.4f}\")\n",
    "    test_acc2a_i.append(test_acc2a)\n",
    "    \n",
    "    test_loss2, test_acc2 = evaluate_model(model2_i[i], X2_test, y_test)\n",
    "    print(f\"Dataset X2 - Test Loss: {test_loss2:.4f}, Test Accuracy: {test_acc2:.4f}\")\n",
    "    test_acc2_i.append(test_acc2)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730f63d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = [160,1600,16000]\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.plot(num_samples,test_acc1_i,'o-',label='inputs')\n",
    "plt.plot(num_samples,test_acc2a_i,'o-',label='embeddings (32)')\n",
    "plt.plot(num_samples,test_acc2_i,'o-',label='embeddings (1024)')\n",
    "plt.plot(num_samples,np.ones(3),'k--')\n",
    "plt.plot(num_samples,0.1*np.ones(3),'k--')\n",
    "plt.xlabel(\"Number of samples\")\n",
    "plt.ylabel(\"Test accuracy\")\n",
    "plt.xscale(\"log\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'figures/test_accuracy_{data_name}_M{margin_str}_ED{ed}.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156cda55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
