{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40855784-cf70-4126-b37c-cc86ad91726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.manifold._t_sne\")\n",
    "\n",
    "import time\n",
    "\n",
    "import itertools\n",
    "#import torch\n",
    "#from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, random_split, Dataset\n",
    "#from torch.utils.data import DataLoader\n",
    "#import numpy as np\n",
    "#import itertools\n",
    "\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfed4927-1af3-439c-a382-72f28f1c4948",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"natural_movies_2025_07_17/natural_movies/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bddf5f92-8b27-4269-9e04-d4dbbebcbef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file path and variable name (adjust as needed)\n",
    "mat_path = file_path + \"triplet_patches.mat\"\n",
    "#mat_path = file_path + \"allPatches.mat\"\n",
    "mat_key = \"allPatches\"  # or whatever the key is in your .mat file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64f233c8-cf62-40f7-962c-c0add016ff1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw HDF5 shape: (100000, 3, 16, 16)\n",
      "Final triplet_data shape: torch.Size([100000, 3, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "# Load from HDF5\n",
    "with h5py.File(mat_path, 'r') as f:\n",
    "    raw = f[mat_key]\n",
    "    print(\"Raw HDF5 shape:\", raw.shape)\n",
    "    data = np.array(raw)  # this ensures a clean ndarray\n",
    "\n",
    "# Transpose if necessary\n",
    "if data.shape == (16, 16, 3, 50000):\n",
    "    data = data.transpose(3, 2, 0, 1)  # -> (50000, 3, 16, 16)\n",
    "\n",
    "# Convert to float32 Torch tensor\n",
    "triplet_data = torch.tensor(data).float() / 255.0\n",
    "\n",
    "# mean = triplet_data.mean(dim=(2, 3), keepdim=True)  # shape (N, C, 1, 1)\n",
    "# std = triplet_data.std(dim=(2, 3), keepdim=True)    # shape (N, C, 1, 1)\n",
    "# triplet_data = (triplet_data - mean) / (std + 1e-8)\n",
    "\n",
    "print(\"Final triplet_data shape:\", triplet_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64ac857f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet Margin Stats:\n",
      "  Mean anchor-positive distance: 3.3578\n",
      "  Mean anchor-negative distance: 5.7631\n",
      "  % Triplets violating margin (0.2): 14.40%\n"
     ]
    }
   ],
   "source": [
    "# Extract anchor, positive, and negative from the triplet tensor\n",
    "anchor   = triplet_data[:, 0, :, :]\n",
    "positive = triplet_data[:, 1, :, :]\n",
    "negative = triplet_data[:, 2, :, :]\n",
    "\n",
    "# Flatten images into vectors\n",
    "a = anchor.view(anchor.size(0), -1)\n",
    "p = positive.view(positive.size(0), -1)\n",
    "n = negative.view(negative.size(0), -1)\n",
    "\n",
    "# Compute distances\n",
    "ap_dist = F.pairwise_distance(a, p)\n",
    "an_dist = F.pairwise_distance(a, n)\n",
    "\n",
    "# Compute margin violations\n",
    "margin = 0.2\n",
    "violations = (ap_dist + margin > an_dist).float()\n",
    "\n",
    "# Compute stats\n",
    "mean_ap = ap_dist.mean().item()\n",
    "mean_an = an_dist.mean().item()\n",
    "violation_rate = violations.mean().item()\n",
    "\n",
    "# Print results\n",
    "print(\"Triplet Margin Stats:\")\n",
    "print(f\"  Mean anchor-positive distance: {mean_ap:.4f}\")\n",
    "print(f\"  Mean anchor-negative distance: {mean_an:.4f}\")\n",
    "print(f\"  % Triplets violating margin ({margin}): {violation_rate * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9cac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# Assume triplet_data is a tensor of shape (N, 3, H, W)\n",
    "# Convert to numpy if needed\n",
    "X_np = triplet_data.numpy() if isinstance(triplet_data, torch.Tensor) else triplet_data\n",
    "\n",
    "# Extract positives and negatives\n",
    "positives = X_np[:, 1].reshape(X_np.shape[0], -1)  # shape: (N, H*W)\n",
    "negatives = X_np[:, 2].reshape(X_np.shape[0], -1)\n",
    "\n",
    "# Stack data and labels\n",
    "X = np.vstack([positives, negatives])  # shape: (2N, H*W)\n",
    "y = np.hstack([np.ones(len(positives)), np.zeros(len(negatives))])  # shape: (2N,)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train logistic regression\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "y_score = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_score)\n",
    "\n",
    "print(f\"üîç Logistic Regression Accuracy: {acc:.4f}\")\n",
    "print(f\"üìà ROC AUC Score: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672de0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_index(x):\n",
    "    x = np.array(x, dtype=np.float64)\n",
    "    if np.amin(x) < 0:\n",
    "        raise ValueError(\"Gini index is only defined for non-negative values\")\n",
    "    if np.all(x == 0):\n",
    "        return 0.0  # Convention: Gini is 0 for uniform zero vector\n",
    "\n",
    "    x_sorted = np.sort(x)\n",
    "    n = len(x)\n",
    "    index = np.arange(1, n + 1)\n",
    "    return (2 * np.sum(index * x_sorted) / np.sum(x)) / n - (n + 1) / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7513448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_triplet_margin_stats(model, dataloader, device, margin=0.2):\n",
    "\n",
    "    model.eval()\n",
    "    ap_dists = []\n",
    "    an_dists = []\n",
    "    violations = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for a, p, n in dataloader:\n",
    "            a, p, n = a.to(device), p.to(device), n.to(device)\n",
    "\n",
    "            anchor = model(a)\n",
    "            positive = model(p)\n",
    "            negative = model(n)\n",
    "\n",
    "            ap = F.pairwise_distance(anchor, positive)\n",
    "            an = F.pairwise_distance(anchor, negative)\n",
    "\n",
    "            ap_dists.append(ap)\n",
    "            an_dists.append(an)\n",
    "            violations.append((ap + margin > an).float())\n",
    "\n",
    "    ap_dists = torch.cat(ap_dists)\n",
    "    an_dists = torch.cat(an_dists)\n",
    "    violations = torch.cat(violations)\n",
    "\n",
    "    mean_ap = ap_dists.mean().item()\n",
    "    mean_an = an_dists.mean().item()\n",
    "    violation_rate = violations.mean().item()\n",
    "\n",
    "    return mean_ap, mean_an, violation_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7befba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch.utils.data import Dataset\n",
    "\n",
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, triplet_tensor):\n",
    "        self.triplets = triplet_tensor  # could be a Tensor or a Subset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        triplet = self.triplets[idx]\n",
    "        anchor = triplet[0].unsqueeze(0)  # shape: (1, 16, 16)\n",
    "        positive = triplet[1].unsqueeze(0)\n",
    "        negative = triplet[2].unsqueeze(0)\n",
    "        return anchor, positive, negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fb457a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Split into train / val / test\n",
    "N = len(triplet_data)\n",
    "train_size = int(0.7 * N)\n",
    "val_size = int(0.1 * N)\n",
    "test_size = N - train_size - val_size\n",
    "train_data, val_data, test_data = random_split(triplet_data, [train_size, val_size, test_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(TripletDataset(train_data), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TripletDataset(val_data), batch_size=batch_size)\n",
    "test_loader = DataLoader(TripletDataset(test_data), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f614db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self, embedding_dim=32):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(16*16, embedding_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        out = self.relu(self.fc(x))  # Linear followed by ReLU\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d3eac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loss(model, data_loader, criterion, device, l1_lambda=0.0):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_triplet = 0.0\n",
    "    total_l1 = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for a, p, n in data_loader:\n",
    "            a, p, n = a.to(device), p.to(device), n.to(device)\n",
    "            a_out, p_out, n_out = model(a), model(p), model(n)\n",
    "\n",
    "            triplet_loss = criterion(a_out, p_out, n_out)\n",
    "\n",
    "            l1_penalty = 0.0\n",
    "            if l1_lambda > 0:\n",
    "                l1_penalty = (\n",
    "                    a_out.abs().sum() +\n",
    "                    p_out.abs().sum() +\n",
    "                    n_out.abs().sum()\n",
    "                ) / a_out.shape[0]\n",
    "\n",
    "            loss = triplet_loss + l1_lambda * l1_penalty\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_triplet += triplet_loss.item()\n",
    "            total_l1 += l1_penalty\n",
    "\n",
    "    n_batches = len(data_loader)\n",
    "    return (\n",
    "        total_loss / n_batches,\n",
    "        total_triplet / n_batches,\n",
    "        total_l1 / n_batches\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25491a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel2d(kernel_size=5, sigma=0.25, device='cpu'):\n",
    "    ax = torch.arange(kernel_size, device=device) - kernel_size // 2\n",
    "    xx, yy = torch.meshgrid(ax, ax, indexing=\"xy\")\n",
    "    kernel = torch.exp(-(xx**2 + yy**2) / (2 * sigma**2))\n",
    "    kernel /= kernel.sum()\n",
    "    return kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84af563a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_weights(model, sigma=0.25):\n",
    "    # Get weights and reshape (embedding_dim, 256) -> (embedding_dim, 1, 16, 16)\n",
    "    weight = model.fc.weight.data\n",
    "    emb_dim = weight.shape[0]\n",
    "    weight_reshaped = weight.view(emb_dim, 1, 16, 16)\n",
    "\n",
    "    # Build Gaussian kernel\n",
    "    kernel_size = int(2 * round(3 * sigma) + 1)\n",
    "    kernel = gaussian_kernel2d(kernel_size, sigma, device=weight.device)\n",
    "    kernel = kernel.view(1, 1, kernel_size, kernel_size)\n",
    "\n",
    "    # Convolve all embeddings in one pass (treat each as separate batch)\n",
    "    smoothed = F.conv2d(weight_reshaped, kernel, padding=kernel_size // 2)\n",
    "\n",
    "    # Flatten back and copy into model\n",
    "    model.fc.weight.data.copy_(smoothed.view(emb_dim, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130d8203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, optimizer, criterion, device, \n",
    "          embedding_dim, epochs=10, patience=5, min_delta=1e-4, l1_lambda=0.0,\n",
    "         margin=0.2):\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    train_triplet_losses, val_triplet_losses = [], []\n",
    "    train_l1_norms, val_l1_norms = [], []\n",
    "    \n",
    "    train_violations, val_violations = [], []\n",
    "    \n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_state = None\n",
    "    best_epoch = 0\n",
    "    wait = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    # ---- Initial evaluation before training ----\n",
    "    train_loss, train_triplet_loss, train_l1_norm = evaluate_loss(model, train_loader,\n",
    "                                                            criterion, device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_triplet_losses.append(train_triplet_loss)\n",
    "    train_l1_norms.append(train_l1_norm)\n",
    "    \n",
    "    _, _, v_train = compute_triplet_margin_stats(model, train_loader, device)\n",
    "    train_violations.append(v_train)\n",
    "\n",
    "    val_loss, val_triplet_loss, val_l1_norm = evaluate_loss(model, val_loader,\n",
    "                                                            criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_triplet_losses.append(val_triplet_loss)\n",
    "    val_l1_norms.append(val_l1_norm)\n",
    "    \n",
    "    _, _, v_val = compute_triplet_margin_stats(model, val_loader, device)\n",
    "    val_violations.append(v_val)\n",
    "\n",
    "    best_val_loss = val_losses[-1]\n",
    "    best_state = model.state_dict()\n",
    "\n",
    "    # ---- Training loop ----\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_triplet = 0.0\n",
    "        total_l1 = 0.0\n",
    "        \n",
    "        for a, p, n in train_loader:\n",
    "            a, p, n = a.to(device), p.to(device), n.to(device)\n",
    "            a_out, p_out, n_out = model(a), model(p), model(n)\n",
    "            triplet_loss = criterion(a_out, p_out, n_out)\n",
    "\n",
    "            l1_penalty = 0.0\n",
    "            if l1_lambda > 0:\n",
    "                l1_penalty = (\n",
    "                    a_out.abs().sum() +\n",
    "                    p_out.abs().sum() +\n",
    "                    n_out.abs().sum()\n",
    "                ) / a_out.shape[0]\n",
    "\n",
    "            loss = triplet_loss + l1_lambda * l1_penalty\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_triplet += triplet_loss.item()\n",
    "            total_l1 += l1_penalty\n",
    "\n",
    "        \n",
    "            \n",
    "        train_losses.append(total_loss / len(train_loader))\n",
    "        train_triplet_losses.append(total_triplet / len(train_loader))\n",
    "        train_l1_norms.append(total_l1 / len(train_loader))\n",
    "        \n",
    "        smooth_weights(model, sigma=0.25)\n",
    "        \n",
    "        # compute violations\n",
    "        _, _, v_train = compute_triplet_margin_stats(model, train_loader, device)\n",
    "        train_violations.append(v_train)\n",
    "\n",
    "        val_loss, val_triplet_loss, val_l1_norm = evaluate_loss(model, val_loader, criterion, device, l1_lambda)\n",
    "        val_losses.append(val_loss)\n",
    "        val_triplet_losses.append(val_triplet_loss)\n",
    "        val_l1_norms.append(val_l1_norm)\n",
    "        _, _, v_val = compute_triplet_margin_stats(model, val_loader, device)\n",
    "        val_violations.append(v_val)\n",
    "\n",
    "        if val_loss < best_val_loss - min_delta:\n",
    "            best_val_loss = val_loss\n",
    "            best_state = model.state_dict()\n",
    "            best_epoch = epoch + 1\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"\\n Early stopping at epoch {epoch + 1}\")\n",
    "                break\n",
    "\n",
    "        print('.', end='', flush=True)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f' Epoch {epoch + 1}')\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTraining completed in {total_time:.2f} seconds\")\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "\n",
    "        \n",
    "    # Save model state separately\n",
    "    #lambda_str = \"0\" if l1_lambda == 0 else f\"{l1_lambda:.0e}\".replace('-', 'm')\n",
    "    margin_str = f\"{margin:.1f}\".replace('.', 'p')\n",
    "    model_path = f\"models/linear_M{margin_str}_ED{embedding_dim}_EP{epochs}.pt\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    # Save metadata separately\n",
    "    meta_path = f\"models/linear_M{margin_str}_ED{embedding_dim}_EP{epochs}.pkl\"\n",
    "    with open(meta_path, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            \"train_losses\": train_losses,\n",
    "            \"val_losses\": val_losses,\n",
    "            \"train_triplet_losses\": train_triplet_losses,\n",
    "            \"val_triplet_losses\": val_triplet_losses,\n",
    "            \"train_l1_norms\": train_l1_norms,\n",
    "            \"val_l1_norms\": val_l1_norms,\n",
    "            \"train_viol\": train_violations,\n",
    "            \"val_viol\": val_violations,\n",
    "            \"train_time\": total_time,\n",
    "            \"best_epoch\": best_epoch\n",
    "        }, f)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe64def7",
   "metadata": {},
   "outputs": [],
   "source": [
    "4096*8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d41160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed hyperparameters\n",
    "epochs = 200\n",
    "patience = 5\n",
    "min_delta = 1e-4\n",
    "\n",
    "# --- Hyperparameter grid ---\n",
    "learning_rate = [1e-4]\n",
    "weight_decays = [0]\n",
    "batch_sizes = [64]\n",
    "\n",
    "l1_lambdas = [0]\n",
    "# embedding_dims = [8, 16, 32, 64, 128, 256, 512]\n",
    "# margins = [0.1, 0.2, 0.3, 0.4]\n",
    "\n",
    "# embedding_dims = [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]\n",
    "# margins = [0.25, 0.5, 1., 2., 4.]\n",
    "\n",
    "embedding_dims = [4, 8, 16384, 32768]\n",
    "margins = [1.]\n",
    "\n",
    "hyper_params = []\n",
    "model_id = 0\n",
    "\n",
    "\n",
    "for lr, wd, bs, ed, ll, mm in itertools.product(learning_rate, weight_decays, batch_sizes,\\\n",
    "                                            embedding_dims, l1_lambdas, margins):\n",
    "    hyper_params.append({\n",
    "        \"model id\": model_id,\n",
    "        \"learning rate\": lr,\n",
    "        \"weight decay\": wd,\n",
    "        \"batch size\": bs,\n",
    "        \"embedding dim\": ed,\n",
    "        \"l1 lambda\": ll,\n",
    "        \"margin\": mm,\n",
    "        \"epochs\": epochs,\n",
    "        \"patience\": patience,\n",
    "        \"min delta\": min_delta,\n",
    "    })\n",
    "\n",
    "    \n",
    "n_models = len(hyper_params)\n",
    "print(n_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1ffe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_id in range(n_models):\n",
    "    lr = hyper_params[model_id][\"learning rate\"]\n",
    "    wd = hyper_params[model_id][\"weight decay\"]\n",
    "    bs = hyper_params[model_id][\"batch size\"]\n",
    "    ed = hyper_params[model_id][\"embedding dim\"]\n",
    "    ll = hyper_params[model_id][\"l1 lambda\"]\n",
    "    epochs = hyper_params[model_id][\"epochs\"]\n",
    "    margin = hyper_params[model_id][\"margin\"]\n",
    "    patience = hyper_params[model_id][\"patience\"]\n",
    "    min_delta = hyper_params[model_id][\"min delta\"]\n",
    "    #lambda_str = \"0\" if ll == 0 else f\"{ll:.0e}\".replace('-', 'm')\n",
    "    margin_str = f\"{margin:.1f}\".replace('.', 'p')\n",
    "    hyper_path = f\"models/linear_hyper_M{margin_str}_ED{ed}_EP{epochs}.pkl\"\n",
    "    with open(hyper_path, 'wb') as f:\n",
    "        pickle.dump({\n",
    "        \"model id\": model_id,\n",
    "        \"learning rate\": lr,\n",
    "        \"weight decay\": wd,\n",
    "        \"batch size\": bs,\n",
    "        \"embedding dim\": ed,\n",
    "        \"l1 lambda\": ll,\n",
    "        \"epochs\": epochs,\n",
    "        \"margin\": margin,\n",
    "        \"patience\": patience,\n",
    "        \"min delta\": min_delta,\n",
    "        }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e884e039",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Iterate over all combinations ---\n",
    "for model_id in range(n_models):\n",
    "    learning_rate = hyper_params[model_id][\"learning rate\"]\n",
    "    wd = hyper_params[model_id][\"weight decay\"]\n",
    "    bs = hyper_params[model_id][\"batch size\"]\n",
    "    ed = hyper_params[model_id][\"embedding dim\"]\n",
    "    ll = hyper_params[model_id][\"l1 lambda\"]\n",
    "    epochs = hyper_params[model_id][\"epochs\"]\n",
    "    margin = hyper_params[model_id][\"margin\"]\n",
    "    patience = hyper_params[model_id][\"patience\"]\n",
    "    min_delta = hyper_params[model_id][\"min delta\"]\n",
    "    \n",
    "    #lambda_str = \"0\" if ll == 0 else f\"{ll:.0e}\".replace('-', 'm')\n",
    "    #margin_str = f\"{margin:.1f}\".replace('.', 'p')\n",
    "    print(f\"\\nTraining {model_id+1}/{n_models} with M={margin}, ED={ed}, WD={wd}, BS={bs}\")\n",
    "    # Dataloaders\n",
    "    train_loader = DataLoader(TripletDataset(train_data), batch_size=bs, shuffle=True)\n",
    "    val_loader = DataLoader(TripletDataset(val_data), batch_size=bs)\n",
    "\n",
    "    model = EmbeddingNet(embedding_dim=ed)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=wd)\n",
    "    criterion = nn.TripletMarginLoss(margin=margin)\n",
    "\n",
    "    model = train(model,train_loader,val_loader,\n",
    "                    optimizer,criterion,device,\n",
    "                    embedding_dim=ed, epochs=epochs,patience=patience,\n",
    "                      min_delta=min_delta, l1_lambda=ll,margin=margin)\n",
    "\n",
    "print(f\"Grid completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e56d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for hp in hyper_params:  # assuming hyper_params is a list of dicts with 'hidden dim' and 'embedding dim'\n",
    "    embedding_dim = hp['embedding dim']\n",
    "    margin = hp['margin']\n",
    "    #lambda_str = \"0\" if l1_lambda == 0 else f\"{l1_lambda:.0e}\".replace('-', 'm')\n",
    "    margin_str = f\"{margin:.1f}\".replace('.', 'p')\n",
    "    filename = f\"models/linear_M{margin_str}_ED{embedding_dim}_EP{epochs}.pkl\"\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\" File not found: {filename}\")\n",
    "        continue\n",
    "\n",
    "    with open(filename, 'rb') as f:\n",
    "        print(filename)\n",
    "        saved = pickle.load(f)\n",
    "\n",
    "    results.append({\n",
    "        \"train_losses\": saved[\"train_losses\"],\n",
    "        \"val_losses\": saved[\"val_losses\"],\n",
    "        \"train_viol\": saved[\"train_viol\"],\n",
    "        \"val_viol\": saved[\"val_viol\"],\n",
    "        \"train_time\": saved[\"train_time\"],\n",
    "        \"best_epoch\": saved[\"best_epoch\"],        \n",
    "    })\n",
    "\n",
    "print(f\" Loaded {len(results)} models from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84c8a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sort and pick best based on early stopping ---\n",
    "best_id = min(range(len(results)), key=lambda i: min(results[i][\"val_losses\"]))\n",
    "best_result = results[best_id]\n",
    "best_hyperparams = hyper_params[best_id]\n",
    "best_val_loss = min(best_result[\"val_losses\"])\n",
    "\n",
    "print(\"\\n Best config:\")\n",
    "print(f\"   ED={best_hyperparams['embedding dim']}, \"\n",
    "    f\"M={best_hyperparams['margin']}\")\n",
    "print(f\"   Best Validation Loss={best_val_loss:.4f}\")\n",
    "print(f\"   Training Time: {best_result['train_time']:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b80cd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_output_activations(model, model_file, test_loader, device):\n",
    "\n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    out_acts = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for anchor, positive, negative in test_loader:\n",
    "            anchor = anchor.to(device)\n",
    "            positive = positive.to(device)\n",
    "            negative = negative.to(device)\n",
    "\n",
    "            out_anchor = model(anchor)\n",
    "            out_positive = model(positive)\n",
    "            out_negative = model(negative)\n",
    "\n",
    "            out_batch = torch.stack([out_anchor, out_positive, out_negative], dim=1)\n",
    "            out_acts.append(out_batch.cpu())\n",
    "\n",
    "    out_acts = torch.cat(out_acts, dim=0).numpy()\n",
    "    return out_acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98caf4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 20000\n",
    "test_results = []\n",
    "\n",
    "for model_id in range(n_models):\n",
    "    ed = hyper_params[model_id][\"embedding dim\"]\n",
    "    margin = hyper_params[model_id][\"margin\"]\n",
    "    margin_str = f\"{margin:.1f}\".replace('.', 'p')\n",
    "    model_file = f\"models/linear_M{margin_str}_ED{ed}_EP{epochs}.pt\"\n",
    "    print(f\"M{margin_str}, ED{ed}, EP{epochs}\")\n",
    "    if not os.path.exists(model_file):\n",
    "        print(f\" Missing files for ED={ed}\")\n",
    "        continue\n",
    "\n",
    "    model = EmbeddingNet(embedding_dim=ed)\n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    criterion = nn.TripletMarginLoss(margin=margin)\n",
    "    # --- Compute test loss using evaluate_loss ---\n",
    "    test_loss, test_triplet_loss, test_l1_norm = evaluate_loss(model, test_loader,\n",
    "                                                                  criterion, device)\n",
    "\n",
    "    # --- Compute margin violation percentage on test set ---\n",
    "    _, _, test_violation_pct = compute_triplet_margin_stats(model, test_loader,\n",
    "                                                            device, margin=0.2)\n",
    "\n",
    "    # Compute Gini index\n",
    "    out_acts = extract_output_activations(model=model,model_file=model_file,\n",
    "        test_loader=test_loader,device=device)\n",
    "    #print(\"Hidden activations shape:\", out_acts.shape)\n",
    "        \n",
    "    gini_act = np.zeros((n_test,3))\n",
    "    for i in range(n_test):\n",
    "        for j in range(3):\n",
    "            gini_act[i,j] = gini_index(out_acts[i,j,:])\n",
    "    gini_act_m = np.mean(gini_act)\n",
    "    \n",
    "    test_results.append({\n",
    "        \"test_losses\": test_loss,\n",
    "        \"test_triplet_loss\": test_triplet_loss,\n",
    "        \"test_l1_norm\": test_l1_norm,\n",
    "        \"test_viol\": test_violation_pct,\n",
    "        \"test_gini\": gini_act_m,\n",
    "    })\n",
    "    \n",
    "    test_path = f\"models/linear_test_M{margin_str}_ED{ed}_EP{epochs}.pkl\"\n",
    "    with open(test_path, 'wb') as f:\n",
    "        pickle.dump({\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_triplet_loss\": test_triplet_loss,\n",
    "        \"test_l1_norm\": test_l1_norm,\n",
    "        \"test_viol\": test_violation_pct,\n",
    "        \"test_gini\": gini_act_m,\n",
    "        }, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2f437f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_param(val):\n",
    "    if val == 0:\n",
    "        return \"0\"\n",
    "    elif val < 1:\n",
    "        return f\"{val:.0e}\"\n",
    "    else:\n",
    "        return str(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0a1e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min(result['best_epoch']  for result in results))\n",
    "print(max(result['best_epoch']  for result in results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4f176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"losses\"\n",
    "metric = \"viol\"\n",
    "\n",
    "min_loss = min(min(r[\"train_\"+metric] + r[\"val_\"+metric]) for r in results)\n",
    "max_loss = max(max(r[\"train_\"+metric] + r[\"val_\"+metric]) for r in results)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "ip = 0\n",
    "for model_id in range(12*6):\n",
    "    ip += 1\n",
    "    plt.subplot(4,8,ip)\n",
    "    mm = hyper_params[model_id][\"margin\"]\n",
    "    ed = hyper_params[model_id][\"embedding dim\"]\n",
    "\n",
    "    \n",
    "    hyper_p = f\"ED={format_param(ed)}, M={format_param(mm)}\"\n",
    "    \n",
    "\n",
    "    plt.plot(results[best_id]['train_'+metric],'k--',alpha=0.5)\n",
    "    plt.plot(results[best_id]['val_'+metric],'k-',alpha=0.5)\n",
    "    plt.plot(results[best_id]['best_epoch'],test_results[best_id]['test_'+metric],'ko',alpha=0.5)\n",
    "\n",
    "    plt.plot(results[model_id]['train_'+metric],'b--')\n",
    "    plt.plot(results[model_id]['val_'+metric],'b-')\n",
    "    plt.plot(results[model_id]['best_epoch'],test_results[model_id]['test_'+metric],'bo')\n",
    "    plt.ylim(min_loss,max_loss)\n",
    "    plt.title(hyper_p)\n",
    "    #plt.yscale('log')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ede0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_i = [\"viol\",\"losses\",\"gini\"]\n",
    "#metric = \"viol\"\n",
    "#metric = \"losses\"\n",
    "#metric = \"gini\"\n",
    "\n",
    "xvar = 'embedding dim'\n",
    "yvar = 'margin'\n",
    "xvals = sorted(set(r[xvar] for r in hyper_params))\n",
    "yvals = sorted(set(r[yvar] for r in hyper_params))\n",
    "\n",
    "# Define styles\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple']\n",
    "yvals_to_color = {ed: colors[i % len(colors)] for i, ed in enumerate(yvals)}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(9, 3))\n",
    "ip = 0\n",
    "for metric in metric_i:\n",
    "    ip += 1\n",
    "    plt.subplot(1,3,ip)\n",
    "    \n",
    "    xval_array = np.array(xvals)\n",
    "\n",
    "    for yval in yvals:\n",
    "        \n",
    "\n",
    "        test_losses = []\n",
    "        for xval in xvals:\n",
    "            model_id = next(i for i, r in enumerate(hyper_params)\n",
    "                            if r[xvar] == xval and r[yvar] == yval)\n",
    "\n",
    "            test_losses.append(test_results[model_id]['test_'+metric])\n",
    "        test_losses = np.array(test_losses)\n",
    "        test_losses /= test_losses[0]\n",
    "        color = yvals_to_color[yval]\n",
    "        label = f\"{yvar}={yval}\"\n",
    "        plt.plot(xval_array, test_losses, 'o-',label=label,alpha=0.5)\n",
    "        # Formatting\n",
    "        plt.xscale(\"log\")\n",
    "        plt.xlabel(xvar)\n",
    "        plt.ylabel(metric)\n",
    "        plt.title(label)\n",
    "        plt.legend(fontsize=6)\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeda2cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_i = [\"viol\",\"losses\",\"gini\"]\n",
    "#metric = \"viol\"\n",
    "#metric = \"losses\"\n",
    "#metric = \"gini\"\n",
    "mouse_exp = 20*16*16\n",
    "human_exp = 300*16*16\n",
    "\n",
    "xvar = 'embedding dim'\n",
    "yvar = 'margin'\n",
    "xvals = sorted(set(r[xvar] for r in hyper_params))\n",
    "yvals = sorted(set(r[yvar] for r in hyper_params))\n",
    "\n",
    "# Define styles\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple']\n",
    "yvals_to_color = {ed: colors[i % len(colors)] for i, ed in enumerate(yvals)}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "ip = 0\n",
    "for metric in metric_i:\n",
    "    \n",
    "    \n",
    "    xval_array = np.array(xvals)\n",
    "\n",
    "    for yval in [4.0]:\n",
    "        ip += 1\n",
    "        plt.subplot(3,5,ip)\n",
    "\n",
    "        test_losses = []\n",
    "        for xval in xvals:\n",
    "            model_id = next(i for i, r in enumerate(hyper_params)\n",
    "                            if r[xvar] == xval and r[yvar] == yval)\n",
    "\n",
    "            test_losses.append(test_results[model_id]['test_'+metric])\n",
    "\n",
    "        color = yvals_to_color[yval]\n",
    "        label = f\"{yvar}={yval}\"\n",
    "        plt.plot(xval_array, test_losses, 'o-',label=label,alpha=0.5)\n",
    "        yl = plt.ylim()  # get current y-axis limits\n",
    "        plt.plot(mouse_exp*np.ones(2), yl, '--r', linewidth=2)  # vertical line at x=3\n",
    "        #plt.plot(human_exp*np.ones(2), yl, '-.g', linewidth=2)  # vertical line at x=7\n",
    "        # Formatting\n",
    "        plt.xscale(\"log\")\n",
    "        plt.xlabel(\"Embedding dimension\")\n",
    "        if metric == \"viol\":\n",
    "            plt.ylabel(\"Fraction violations\")\n",
    "        elif metric == \"losses\":\n",
    "            plt.ylabel(\"Triplet loss\")\n",
    "        elif metric == \"gini\":\n",
    "            plt.ylabel(\"Gini index\")\n",
    "\n",
    "        \n",
    "        #plt.title(label)\n",
    "        #plt.legend()\n",
    "        plt.tight_layout()\n",
    "plt.savefig('figures/test_results_margin4.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddf7eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07c357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_i = [\"viol\",\"losses\",\"gini\"]\n",
    "#metric = \"viol\"\n",
    "#metric = \"losses\"\n",
    "#metric = \"gini\"\n",
    "mouse_exp = 20*16*16\n",
    "human_exp = 300*16*16\n",
    "\n",
    "xvar = 'embedding dim'\n",
    "yvar = 'margin'\n",
    "xvals = sorted(set(r[xvar] for r in hyper_params))\n",
    "yvals = sorted(set(r[yvar] for r in hyper_params))\n",
    "\n",
    "# Define styles\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple']\n",
    "yvals_to_color = {ed: colors[i % len(colors)] for i, ed in enumerate(yvals)}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(9, 6))\n",
    "ip = 0\n",
    "for metric in metric_i:\n",
    "    \n",
    "    \n",
    "    xval_array = np.array(xvals)\n",
    "\n",
    "    for yval in yvals:\n",
    "        ip += 1\n",
    "        plt.subplot(3,5,ip)\n",
    "\n",
    "        test_losses = []\n",
    "        for xval in xvals:\n",
    "            model_id = next(i for i, r in enumerate(hyper_params)\n",
    "                            if r[xvar] == xval and r[yvar] == yval)\n",
    "\n",
    "            test_losses.append(test_results[model_id]['test_'+metric])\n",
    "\n",
    "        color = yvals_to_color[yval]\n",
    "        label = f\"{yvar}={yval}\"\n",
    "        plt.plot(xval_array, test_losses, 'o-',label=label,alpha=0.5)\n",
    "        yl = plt.ylim()  # get current y-axis limits\n",
    "        plt.plot(mouse_exp*np.ones(2), yl, '--r', linewidth=2)  # vertical line at x=3\n",
    "        #plt.plot(human_exp*np.ones(2), yl, '-.g', linewidth=2)  # vertical line at x=7\n",
    "        # Formatting\n",
    "        plt.xscale(\"log\")\n",
    "        plt.xlabel(xvar)\n",
    "        plt.ylabel(metric)\n",
    "        plt.title(label)\n",
    "        #plt.legend()\n",
    "        plt.tight_layout()\n",
    "plt.savefig('figures/test_results.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea97edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange_blocks_fixed(blocks, p, q):\n",
    "    m, n1, n2 = blocks.shape\n",
    "    assert n1 == n2, \"Blocks must be square\"\n",
    "    assert m == p * q, \"m must equal p * q\"\n",
    "    n = n1\n",
    "    reshaped = blocks.reshape(p, q, n, n)\n",
    "    merged = reshaped.swapaxes(1, 2).reshape(p * n, q * n)\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b1f581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resmooth_weights(model, sigma=0.25):\n",
    "    weight = model.fc.weight.data\n",
    "    emb_dim = weight.shape[0]\n",
    "    weight_reshaped = weight.view(emb_dim, 1, 16, 16)\n",
    "\n",
    "    kernel_size = int(2 * round(3 * sigma) + 1)\n",
    "    kernel = gaussian_kernel2d(kernel_size, sigma, device=weight.device)\n",
    "    kernel = kernel.view(1, 1, kernel_size, kernel_size)\n",
    "\n",
    "    smoothed = F.conv2d(weight_reshaped, kernel, padding=kernel_size // 2)\n",
    "    model.fc.weight.data.copy_(smoothed.view(emb_dim, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de3918d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "weights_reshaped = []\n",
    "for model_id in range(n_models):\n",
    "\n",
    "    ed = hyper_params[model_id][\"embedding dim\"]\n",
    "\n",
    "    margin = hyper_params[model_id][\"margin\"]\n",
    "    margin_str = f\"{margin:.1f}\".replace('.', 'p')\n",
    "    model_file = f\"models/linear_M{margin_str}_ED{ed}_EP{epochs}.pt\"\n",
    "    model = EmbeddingNet(embedding_dim=ed)\n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # ---- Smooth the weights again ----\n",
    "    resmooth_weights(model, sigma=1.)\n",
    "\n",
    "    # Extract weights from first layer (input to hidden): shape (n_hidden_units, 256)\n",
    "    first_layer = model.fc  # Replace if you used a different name\n",
    "    weights = first_layer.weight.detach().cpu().numpy()  # shape: (n_hidden_units, 256)\n",
    "\n",
    "    # Reshape to (n_hidden_units, 16, 16)\n",
    "    weights_reshaped.append(weights.reshape(weights.shape[0], 16, 16))\n",
    "\n",
    "    #print(\"Weight shape:\", weights_reshaped[model_id].shape)  # (n_hidden_units, 16, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b28dabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "n = 16\n",
    "p = 4 # rows \n",
    "q = 8 # columns\n",
    "size_factor = 0.5\n",
    "for model_id in range(n_models):\n",
    "    \n",
    "    embedding_dim = hyper_params[model_id]['embedding dim']\n",
    "    margin = hyper_params[model_id]['margin']\n",
    "    # ---- Filter ----\n",
    "    if margin != 4.0 or embedding_dim not in [32, 512, 4096]:\n",
    "        continue  # skip models that don't match criteria\n",
    "    \n",
    "    merged_rf = rearrange_blocks_fixed(weights_reshaped[model_id][:32,:,:], p=p, q=q)\n",
    "    plt.figure(figsize=(q*size_factor,p*size_factor))\n",
    "    plt.imshow(merged_rf)\n",
    "    for i in range(q+1):\n",
    "        plt.plot(i*n*np.ones(2)-0.5,np.array([0,n*p])-0.5,'w')\n",
    "    for i in range(p+1):\n",
    "        plt.plot(np.array([0,n*q])-0.5,i*n*np.ones(2)-0.5,'w')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    embedding_dim = hyper_params[model_id]['embedding dim']\n",
    "    margin = hyper_params[model_id]['margin']\n",
    "    #lambda_str = \"0\" if l1_lambda == 0 else f\"{l1_lambda:.0e}\".replace('-', 'm')\n",
    "    plt.title(f\"M={margin}, ED={embedding_dim}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'figures/RF_ED{embedding_dim}.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aed7d9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2420037",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
